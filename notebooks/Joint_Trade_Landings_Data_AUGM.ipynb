{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model data for shark and ray meat landings and trade applied to 2012-2019 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Model for shark and ray meat landings and trade applied to 2014-2019 data\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pyt\n",
    "import rdata as rd\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import scipy as sp\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set figure style.\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "bd = os.getcwd() + \"/../Data/\"\n",
    "\n",
    "# Helper functions\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo, Ix\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "match = lambda a, b: np.array([b.index(x) if x in b else None for x in a])\n",
    "\n",
    "\n",
    "def unique(series: pd.Series):\n",
    "    \"Helper function to sort and isolate unique values of a Pandas Series\"\n",
    "    return series.sort_values().unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load landings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[83]:\n",
    "\n",
    "#'''\n",
    "#dnam = bd + \"/fishorshark/modeldatsharksraysImportNeg1augmented\"\n",
    "dnam = bd + \"modeldat_Augmented20250116_CHNupdates\"\n",
    "parsed = rd.parser.parse_file(dnam + \".RData\")\n",
    "converted = rd.conversion.convert(parsed)\n",
    "tmp = converted[\"modeldat\"]\n",
    "#'''\n",
    "#'''\n",
    "# Matrices\n",
    "speciesCountryIDMap = (\n",
    "    tmp[\"speciesCountryIDMap\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\", \"dim_2\": \"taxon\"})\n",
    "    .transpose(\"country\", \"species\", \"taxon\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "logProbPrior = (\n",
    "    tmp[\"logProbPrior\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\", \"dim_2\": \"taxon\"})\n",
    "    .transpose(\"country\", \"species\", \"taxon\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "priorImportance = (\n",
    "    tmp[\"priorImportance\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "OutputSpeciesCountryMapFull = (\n",
    "    tmp[\"OutputSpeciesCountryMapFull\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "OutputSpeciesCountryMap = (\n",
    "    tmp[\"OutputSpeciesCountryMap\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "SpeciesCommodityMap = (\n",
    "    tmp[\"speciesCommodities\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"commodity\"})\n",
    "    .sortby(\"species\")\n",
    ")\n",
    "\n",
    "# Grab key for shark/ray\n",
    "srkey = pd.read_csv(bd + \"taxonomy_20240205.csv\")\n",
    "srkey['group'] = srkey.Superorder.replace('Batoidea','rays').replace('Selachimorpha','sharks').to_numpy()\n",
    "\n",
    "# Trade covariate\n",
    "tradeImportance = (\n",
    "    tmp[\"tradeImportance\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "# Observed landings\n",
    "ObsLandings_ = pd.melt(tmp[\"national_landings_info\"], id_vars='country', value_name='landings')\n",
    "ObsLandings_.columns = ['country','species','obs_landings']\n",
    "ObsLandings_.species = np.array([s.replace('.' , ' ') for s in ObsLandings_.species.values])\n",
    "ObsLandings_ = ObsLandings_.set_index(['country','species']).to_xarray().to_dataarray()[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified index and columns\n",
    "scorez = list(np.unique(priorImportance).astype(int).astype(str))\n",
    "tradeweightcounts = pd.DataFrame(index=priorImportance.country, columns=scorez).fillna(0)\n",
    "for c in priorImportance.country.to_numpy():\n",
    "    colz, valz = np.unique(priorImportance[priorImportance.country==c,:],return_counts=True)\n",
    "    tradeweightcounts.loc[c,colz.astype(int).astype(str)] = valz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tradeweightcounts_init = tradeweightcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tradeweightcounts_init"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Species checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "priorImportance.loc[:,priorImportance.species=='Fontitrygon garouaensis'] = -999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure available taxon matches for reported species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize species to taxon mapping\n",
    "SpeciesTaxonMAP = speciesCountryIDMap[0].drop_vars('country')\n",
    "#np.unique(SpeciesTaxonMAP, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Match taxa to species level regardless of taxonomic level of aggregation\n",
    "for t in speciesCountryIDMap.taxon.values:\n",
    "    # Iterate over possible species for each taxon\n",
    "    for s in srkey[srkey.isin([t]).any(axis=1)]['species_binomial'].values:\n",
    "        try:\n",
    "            SpeciesTaxonMAP.loc[dict(species=s,taxon=t)] = 1\n",
    "        except:\n",
    "            pass\n",
    "#np.unique(SpeciesTaxonMAP, return_counts=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Species cutoff\n",
    "sppcutoff = -1\n",
    "# List of rarely caught species \n",
    "drop_spp = priorImportance.species.to_numpy()[priorImportance.max([\"country\"])<sppcutoff]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Number of species remaining with prior importance less than or equal to cutoff removed\n",
    "#priorImportance.species.shape[0]-drop_spp.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make temporary list of all taxon IDs\n",
    "tmp_taxon_ = speciesCountryIDMap.taxon.to_numpy()\n",
    "# Index taxons relative to what gets landed\n",
    "tmp_TaxonIDx = match(tmp[\"LandingsID\"], list(tmp_taxon_))\n",
    "# Temporary list of all species IDs\n",
    "tmp_species_ = speciesCountryIDMap.species.to_numpy()\n",
    "# Boolean of taxons that are to species level in observed landings\n",
    "tmp_tindx = pd.Series(tmp_taxon_[tmp_TaxonIDx]).str.count(\" \").to_numpy() == 0\n",
    "# Index of species in taxon that are observed as catches\n",
    "tmp_species_spp_id = match(tmp_taxon_[tmp_TaxonIDx[tmp_tindx == 0]], list(tmp_species_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unique names of species in taxon list that are observed as taxon catches but have prior importance below cutoff\n",
    "tmp_spp = np.unique(tmp_species_[tmp_species_spp_id[np.log1p(tmp[\"allCatch\"])[tmp_tindx == 0]<sppcutoff]])\n",
    "#len(tmp_spp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Species in drop list that are actually observed as in taxon+species (taxon) list\n",
    "tmp_spp = list(drop_spp[np.array([x in tmp_spp for x in drop_spp])])\n",
    "#len(tmp_spp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(priorImportance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab landings data to see which taxons have catch\n",
    "tmp_landings = tmp[\"allCatch\"]\n",
    "tmp_taxon = tmp[\"LandingsID\"]\n",
    "tmp_country = tmp[\"country\"]\n",
    "# Iterate over landings to ensure species are avaiable for taxon in country\n",
    "for l,t,c in zip(tmp_landings,tmp_taxon,tmp_country):\n",
    "    # Look for species landed with impossible priors\n",
    "    try:\n",
    "        if (priorImportance.sel(country=c,species=t)==-999)*(l>0):\n",
    "            priorImportance.loc[dict(country=c,species=t)] = sppcutoff\n",
    "            tmp_spp += [t]\n",
    "            #print(\"Changed \"+c+\" \"+t+' to '+str(sppcutoff) )\n",
    "    # Look for taxon landed with no possible species \n",
    "    except:\n",
    "        # Possible species for taxon\n",
    "        tax_spp = (SpeciesTaxonMAP.sel(taxon=t).species[SpeciesTaxonMAP.sel(taxon=t)==1]).values\n",
    "        # If all species are impossible for taxon\n",
    "        if (priorImportance.sel(country=c,species=tax_spp).mean()==-999.)*(l>0):\n",
    "            # Assign to possible species in nation\n",
    "            if OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).sum()>0:\n",
    "                axx = tax_spp[OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).to_numpy()>0]\n",
    "                xflax = 'ok'\n",
    "            else:\n",
    "                # Assign to most likely spp given global max\n",
    "                axx = tax_spp[(priorImportance.sel(species=tax_spp).max('country')==priorImportance.sel(species=tax_spp).max())]\n",
    "                xflax = 'not present'\n",
    "            # Change landings prior to cutoff value\n",
    "            priorImportance.loc[dict(country=c,species=axx)] = sppcutoff\n",
    "            tmp_spp += list(axx)\n",
    "            #print(\"Impossible \"+t+\" changed \"+c+\", spp are \"+xflax)\n",
    "            #print(axx)\n",
    "        \n",
    "        # If all species for taxon are below data-reduction cutoff\n",
    "        elif (priorImportance.sel(country=c,species=tax_spp).max()<sppcutoff)*(l>0):\n",
    "            # Assign to possible species in nation\n",
    "            if OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).sum()>0:\n",
    "                axx = tax_spp[OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).to_numpy()>0]\n",
    "                xflax = 'ok'\n",
    "            else:\n",
    "                # Assign to most likely spp given global max\n",
    "                axx = tax_spp[(priorImportance.sel(species=tax_spp).max('country')==priorImportance.sel(species=tax_spp).max())]\n",
    "                xflax = 'not present'\n",
    "            # Change landings prior to cutoff value\n",
    "            priorImportance.loc[dict(country=c,species=axx)] = sppcutoff\n",
    "            tmp_spp += list(axx)\n",
    "            #print(\"Low \"+t+\" changed \"+c+\", spp are \"+xflax)\n",
    "            #print(axx)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified index and columns\n",
    "scorez = list(np.unique(priorImportance).astype(int).astype(str))\n",
    "tradeweightcounts = pd.DataFrame(index=priorImportance.country, columns=scorez).fillna(0)\n",
    "for c in priorImportance.country.to_numpy():\n",
    "    colz, valz = np.unique(priorImportance[priorImportance.country==c,:],return_counts=True)\n",
    "    tradeweightcounts.loc[c,colz.astype(int).astype(str)] = valz\n",
    "#tradeweightcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add in species in field data\n",
    "field_tmp = np.array(['Atlantoraja cyclhophora', 'Bathtoshia centroura',\n",
    "       'Callorynchus callorynchus',\n",
    "       'Dasyatis hypostigma', 'Fontitrygon geijskesi',\n",
    "       'Hypanus bethalutzae', 'Mobula hypostoma', 'Narcine brasiliensis',\n",
    "       'Pseudobatos horkelii', 'Pteroplatytrygon violacae',\n",
    "       'Rhinoptera brasilisensis', 'Rioraja agassizi',\n",
    "       'Scyliorhinus haekelii', 'Squalus albicaudus', 'Squatina occulta',\n",
    "       'Zapteryx brevirostris','Apristurus brunneus', 'Bathyraja aleutica',\n",
    "       'Bathyraja interrupta', 'Bathyraja murrayi',\n",
    "       'Cephaloscyllium isabellum', 'Gollum attenuatus',\n",
    "       'Oxynotus bruniensis', 'Rostroraja eglanteria'])\n",
    "for s in field_tmp:\n",
    "    if s not in tmp_spp:\n",
    "        tmp_spp+=[s]\n",
    "\n",
    "# Species to drop = have prior importance <=1 AND are not actually observed as taxon to the species level\n",
    "drop_spp = drop_spp[np.array([x not in tmp_spp for x in drop_spp])]\n",
    "\n",
    "#\"\"\"\n",
    "# Drop rare and unreported species\n",
    "speciesCountryIDMap = speciesCountryIDMap.drop_sel(species=drop_spp)\n",
    "priorImportance = priorImportance.drop_sel(species=drop_spp)\n",
    "logProbPrior = logProbPrior.drop_sel(species=drop_spp)\n",
    "SpeciesCommodityMap = SpeciesCommodityMap.drop_sel(species=drop_spp)\n",
    "# Add in unreporting countries\n",
    "\n",
    "# = = = = = = = = = = = = = = After species drop = = = = = = = = = = = = = #\n",
    "\n",
    "# Vectors\n",
    "allCatch = tmp[\"allCatch\"]\n",
    "cindx = allCatch>0\n",
    "allCatch = allCatch[cindx]\n",
    "logCatch = np.log1p(tmp[\"allCatch\"])[cindx]\n",
    "species_ = logProbPrior.species.to_numpy()\n",
    "country_ = logProbPrior.country.to_numpy()\n",
    "CountryIDx = match(tmp[\"country\"][cindx], list(country_))\n",
    "year_ = [\"year_\" + str(x) for x in np.unique(tmp[\"year\"][cindx]).astype(int)]\n",
    "YearIDx = match(tmp[\"year\"][cindx], list(np.unique(tmp[\"year\"][cindx]).astype(int)))\n",
    "taxon_ = logProbPrior.taxon.to_numpy()\n",
    "TaxonIDx = match(tmp[\"LandingsID\"][cindx], list(taxon_))\n",
    "speciesCountryMap = speciesCountryIDMap.groupby(\"species\").max(\"taxon\")\n",
    "TaxonPRIOR = priorImportance.to_numpy()\n",
    "\n",
    "# Meat species\n",
    "#meat_mask = SpeciesCommodityMap.sel(commodity='meat',species=species_).to_numpy()\n",
    "meat_mask = 1*(SpeciesCommodityMap.sel(commodity=('fins'),species=species_)+SpeciesCommodityMap.sel(commodity=('meat'),species=species_)>0).to_numpy()\n",
    "meat_mask[meat_mask==0] = -9\n",
    "meat_mask[meat_mask==1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match FAO to observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset observed landings to match FAO countries\n",
    "Obsspp = np.sort(np.array(list(set(ObsLandings_.species.values).intersection(priorImportance.species.values))))\n",
    "Obscou = np.sort(np.array(list(set(ObsLandings_.country.values).intersection(priorImportance.country.values))))\n",
    "ObsLandings = ObsLandings_.sel(country=Obscou, species=Obsspp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tmp empty data to match dimension of full species compliment for observed countries\n",
    "tmpdata = priorImportance.copy()*-0\n",
    "tmpdata.loc[dict(country=ObsLandings.country, species=ObsLandings.species)] = ObsLandings.values\n",
    "ObsLandings = tmpdata.sel(country=Obscou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average total catch for target of softmax\n",
    "TotalCatch = np.array([allCatch[CountryIDx==i].sum() for i in range(speciesCountryIDMap.shape[0])])/len(year_)\n",
    "logTotalCatch = np.log(TotalCatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#np.unique(taxon_[TaxonIDx[tindx == 0]][match(taxon_[TaxonIDx[tindx == 0]], list(species_))==None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #\n",
    "\n",
    "# Make group vector\n",
    "group_ = srkey.group[match(species_,list(srkey.species_binomial))].to_numpy()\n",
    "# Make masks for trade sums\n",
    "shark_mask = 1*(group_=='sharks')\n",
    "ray_mask = 1*(group_=='rays')\n",
    "\n",
    "# Grab only aggregated taxa\n",
    "taxon_shortlist = taxon_[pd.Series(taxon_).str.count(\" \").to_numpy() == 0]\n",
    "\n",
    "# Split data index\n",
    "tindx = pd.Series(taxon_[TaxonIDx]).str.count(\" \").to_numpy() == 0\n",
    "# Split landings\n",
    "logReported_species_landings = logCatch[tindx == 0]\n",
    "logReported_taxon_landings = logCatch[tindx == 1]\n",
    "# Split country index\n",
    "country_spp_id = CountryIDx[tindx == 0]\n",
    "country_tax_id = CountryIDx[tindx == 1]\n",
    "# Split year index\n",
    "year_spp_id = YearIDx[tindx == 0]\n",
    "year_tax_id = YearIDx[tindx == 1]\n",
    "# Split species index\n",
    "species_spp_id = match(taxon_[TaxonIDx[tindx == 0]], list(species_))\n",
    "# Split taxon index\n",
    "# taxon_tax_id = TaxonIDx[tindx==1]\n",
    "taxon_tax_id = match(taxon_[TaxonIDx[tindx == 1]], list(taxon_shortlist))\n",
    "\n",
    "\n",
    "\n",
    "# Make dataframes for later\n",
    "sdata = pd.DataFrame(\n",
    "    {\n",
    "        \"logReported_species_landings\": logReported_species_landings,\n",
    "        \"species_spp_id\": species_spp_id,\n",
    "        \"country_spp_id\": country_spp_id,\n",
    "        \"year_spp_id\": year_spp_id,\n",
    "        \"country\": country_[country_spp_id],\n",
    "        \"year\": np.array(year_)[year_spp_id],\n",
    "        \"species\": species_[species_spp_id],\n",
    "    }\n",
    ")\n",
    "txdata = pd.DataFrame(\n",
    "    {\n",
    "        \"logReported_taxon_landings\": logReported_taxon_landings,\n",
    "        \"taxon_tax_id\": taxon_tax_id,\n",
    "        \"country_tax_id\": country_tax_id,\n",
    "        \"year_tax_id\": year_tax_id,\n",
    "        \"country\": country_[country_tax_id],\n",
    "        \"year\": np.array(year_)[year_tax_id],\n",
    "        \"taxon\": taxon_shortlist[taxon_tax_id],\n",
    "    }\n",
    ")\n",
    "txdata = txdata.loc[match(txdata.taxon, list(taxon_shortlist)) != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial mask of species matched to possible taxon\n",
    "InitTaxonMASK = speciesCountryIDMap.copy()\n",
    "# Create empty mask to store observed taxa as possible\n",
    "TaxonMASK = InitTaxonMASK*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reported taxa by country\n",
    "Obs_tax_data = np.exp(txdata.drop(columns=['year','year_tax_id','country_tax_id','taxon_tax_id']\n",
    "          ).groupby(['country','taxon']).mean()).rename(columns={\"logReported_taxon_landings\": \"Reported_landings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratre over countries\n",
    "for c in country_:\n",
    "    try:\n",
    "        # Iterate over observed taxa and make possible\n",
    "        taxes = Obs_tax_data.loc[c].index.values\n",
    "        for t in taxes:\n",
    "            for s in species_:\n",
    "                TaxonMASK.loc[dict(country=c,species=s,taxon=t)]=InitTaxonMASK.loc[dict(country=c,species=s,taxon=t)]\n",
    "    except:\n",
    "        #print(c)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update trade and landing weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab trade weights\n",
    "wdata = pd.read_csv(bd + \"trade_weights.csv\").copy()\n",
    "# Empty array to hold values\n",
    "tradeweights = pd.DataFrame(index=country_, columns=species_).fillna(0).to_numpy()\n",
    "tradeweights.flags.writeable = True\n",
    "\n",
    "# Make xarray and add importer dimension\n",
    "tradeweights = (xr.DataArray(tradeweights, dims=('exporter', 'species'))\n",
    "                .assign_coords({\"exporter\": country_,\"species\": species_})\n",
    "                .expand_dims(importer=country_,axis=2)\n",
    "               )\n",
    "# Make a copy to make it writable\n",
    "tradeweights = tradeweights.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fill in values\n",
    "for index, row in wdata.iterrows():\n",
    "    try:\n",
    "        # Common export logOdds\n",
    "        tradeweights.loc[dict(exporter=row.exporter,species=row.species)] = row.tradeweight\n",
    "        # Domestic consumption logOdds\n",
    "        tradeweights.loc[dict(exporter=row.exporter,species=row.species,importer=row.exporter)] = row.domesticweight\n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make priors using relative odds to proportion total landings\n",
    "SppPRIOR = priorImportance.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified index and columns\n",
    "scorez = list(np.unique(SppPRIOR).astype(int).astype(str))\n",
    "tradeweightcounts = pd.DataFrame(index=country_, columns=scorez).fillna(0)\n",
    "\n",
    "for c in country_:\n",
    "    colz, valz = np.unique(SppPRIOR[country_==c,:],return_counts=True)\n",
    "    tradeweightcounts.loc[c,colz.astype(int).astype(str)] = valz\n",
    "#tradeweightcounts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add updates to landing priors - NOV 2024\n",
    "for index, row in wdata.iterrows():\n",
    "    SppPRIOR[country_==row.exporter,species_==row.species] = row.landweight"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Re-weight scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Downweight -2\n",
    "poo = SppPRIOR.copy()\n",
    "poo[poo==-2] = -3\n",
    "# Calculate the expected proportions of species in each country\n",
    "tmp = pd.DataFrame(np.exp(poo))\n",
    "proportions_df = tmp.div(tmp.sum(axis=1), axis=0).fillna(0)\n",
    "# Calculate log-odds for each proportion\n",
    "# Adding a small constant to avoid log(0)\n",
    "epsilon = 1e-6\n",
    "SppPRIORadj = round(np.log(proportions_df / (1 - proportions_df + epsilon) + epsilon)+6).values\n",
    "\n",
    "# Cut down taxon MASK to match taxon_shortlist dimensions\n",
    "#TaxonMASK_S = TaxonMASK[:, :, match(taxon_shortlist, list(taxon_))]\n",
    "TaxonMASK_S = TaxonMASK.sel(taxon=taxon_shortlist)\n",
    "TaxonMASK_Sx = TaxonMASK_S.to_numpy()\n",
    "\n",
    "# Negative mask for log-odds zeros\n",
    "negval = -9\n",
    "TaxonMASK_NEG = TaxonMASK_S.copy().to_numpy()\n",
    "TaxonMASK_NEG[TaxonMASK_NEG==0] = negval\n",
    "\n",
    "\n",
    "# Species weight for countries with no aggregations - huge log-odds so p(species ID)=1 where needed\n",
    "NoTaxaSppWT = np.zeros(SppPRIOR.shape)\n",
    "NoTaxaSppWT[(TaxonMASK_NEG != negval).sum(1).sum(1) == 0, :] = abs(negval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array(['Carcharhinus acronotus', 'Carcharhinus brevipinna',\n",
    "       'Dasyatis hypostigma', 'Heptranchias perlo',\n",
    "       'Narcine brasiliensis', 'Rhinoptera bonasus',\n",
    "       'Rhizoprionodon lalandii'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty DataFrame with the specified index and columns\n",
    "scorez = list(np.unique(SppPRIOR).astype(int).astype(str))\n",
    "tradeweightcounts = pd.DataFrame(index=country_, columns=scorez).fillna(0)\n",
    "\n",
    "for c in country_:\n",
    "    colz, valz = np.unique(SppPRIOR[country_==c,:],return_counts=True)\n",
    "    tradeweightcounts.loc[c,colz.astype(int).astype(str)] = valz\n",
    "#tradeweightcounts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match landings and trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make fdata table to merge with trade model\n",
    "fdata = pd.DataFrame(\n",
    "    {\n",
    "        \"year\": YearIDx + 2012,\n",
    "        \"country_code\": country_[CountryIDx],\n",
    "        \"species\": taxon_[TaxonIDx],\n",
    "        \"landed_weight\": allCatch,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add shark/ray group for each taxon in landings\n",
    "tmp_taxon = fdata.species.unique()\n",
    "tmp_group = []\n",
    "\n",
    "for tx in tmp_taxon:\n",
    "    # taxon at species level\n",
    "    if tx in srkey.species_binomial.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.species_binomial==tx].values[0]]\n",
    "    elif tx in srkey.Genus.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.Genus==tx].values[0]]\n",
    "    elif tx in srkey.Family.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.Family==tx].values[0]]\n",
    "    elif tx in srkey.Order.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.Order==tx].values[0]]\n",
    "    elif tx in ['Sphyrnidae','Selachimorpha']:\n",
    "        tmp_group += ['sharks']\n",
    "    elif tx in ['Elasmobranchii']:\n",
    "        tmp_group += ['elasmos']\n",
    "    else:\n",
    "        print(tx)\n",
    "fdata['group'] = np.array(tmp_group)[match(fdata.species,list(tmp_taxon))]\n",
    "\n",
    "\n",
    "\n",
    "## CHECK THAT ADDITIONAL ELASMOS ARE OK WITH RE-EXPORT CALCULATIONS. \n",
    "## CHECK ALL LANDINGS AND TRADE DATA TO ENSURE SAME YEAR-LEVEL OBSERVATIONS\n",
    "\n",
    "\n",
    "# ## Import commodity code table\n",
    "\n",
    "# Import taxonomic match table for BACI commodity codes and species (MASK)\n",
    "cdata = pd.read_csv(bd + \"comm.code.taxon.match.csv\")\n",
    "\n",
    "# ## Load BACI keys\n",
    "\n",
    "# Import BACI commodity code key\n",
    "ckey = pd.read_csv(bd + \"product_codes_HS12_V202102.csv\")\n",
    "ckey17 = pd.read_csv(bd + \"product_codes_HS17_V202102.csv\")\n",
    "\n",
    "\n",
    "# Import country keys\n",
    "kdata = pd.read_csv(bd + \"country_codes_V202409.csv\")\n",
    "kdata.country_code = kdata.country_code.values.astype(int)\n",
    "\n",
    "# TWN doesn't have an ISO code\n",
    "kdata.loc[\n",
    "    kdata.country_name_full == \"Other Asia, not elsewhere specified\", \"iso_3digit_alpha\"\n",
    "] = \"TWN\"\n",
    "\n",
    "# ## Load BACI seafood trade\n",
    "\n",
    "# Import overall trade from BACI data\n",
    "odata = pd.read_csv(bd + \"baci.seafood_12-19_ij_all.csv\")\n",
    "\n",
    "# Make them numeric\n",
    "odata.exporter_i = odata.exporter_i.values.astype(int)\n",
    "odata.importer_j = odata.importer_j.values.astype(int)\n",
    "\n",
    "# Add country codes\n",
    "\n",
    "odata[\"ISOex_i\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(odata.exporter_i.values), list(kdata.country_code.values))\n",
    "]\n",
    "odata[\"ISOim_j\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(odata.importer_j.values), list(kdata.country_code.values))\n",
    "]\n",
    "\n",
    "\n",
    "# ## Load BACI meat trade\n",
    "\n",
    "# Import BACI data\n",
    "#tdata = pd.read_csv(bd + \"baci.elasmo_HS12_2012-2017.csv\")\n",
    "tdata = pd.read_csv(bd + \"baci_HS12-19_elasmo.csv\")\n",
    "tdata = tdata[tdata['quantity_q'].notna()]\n",
    "tdata = tdata[tdata['quantity_q']!='           NA']\n",
    "\n",
    "# Make them numeric\n",
    "tdata.exporter_i = tdata.exporter_i.values.astype(int)\n",
    "tdata.importer_j = tdata.importer_j.values.astype(int)\n",
    "tdata.quantity_q = tdata.quantity_q.values.astype(float)\n",
    "\n",
    "# Temporary change of code for Italy\n",
    "tdata.loc[tdata.exporter_i == 381, \"exporter_i\"] = 380\n",
    "tdata.loc[tdata.importer_j == 381, \"importer_j\"] = 380\n",
    "\n",
    "# Add country names for imports/exports\n",
    "tdata[\"ISOex_i\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(tdata.exporter_i.values), list(kdata.country_code.values))\n",
    "]\n",
    "tdata[\"ISOim_j\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(tdata.importer_j.values), list(kdata.country_code.values))\n",
    "]\n",
    "\n",
    "# Add explicit code descriptions\n",
    "tdata['hs_description'] = ckey17.description.values[match(tdata[\"hscode_k\"],list(ckey17.code.values))]\n",
    "\n",
    "# Drop all trade on code 30571\n",
    "tdata = tdata[tdata[\"hscode_k\"]!=30571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#list(filter(lambda x: 'fish' in x, ckey.description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make fresh/frozen fins dummy\n",
    "tmp = np.array([0]*len(tdata['hs_description']))\n",
    "tmp[tdata[\"hscode_k\"]==30292] = 1\n",
    "tmp[tdata[\"hscode_k\"]==30392] = 1\n",
    "tdata['fins'] = tmp\n",
    "\n",
    "# Make rays dummy\n",
    "tdata['rays'] = np.array(['rays' in l for l in tdata['hs_description']])*1\n",
    "# Make 30488 (grouped code) sharks\n",
    "maskx = tdata.hscode_k==30488\n",
    "tdata.loc[maskx,'rays'] = 0\n",
    "\n",
    "# Add shark or ray group\n",
    "tmp = np.array(['sharks']*len(tdata['rays']))\n",
    "tmp[tdata['rays']==1] = 'rays'\n",
    "tdata['group'] = tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weight for fins"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab sharks in years that have fresh/frozen fins separated\n",
    "tdata_17 = tdata.copy()[(tdata.year_t>=tdata[tdata.fins==1].year_t.min()) & (tdata.group=='sharks')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total exports of fins and meat per year, exporter, importer combo\n",
    "tmp = tdata_17.groupby([\"ISOex_i\",\"ISOim_j\",\"year_t\",\"fins\"]).sum().reset_index()\n",
    "# Grab 30292 and 30392 (fins) commodity\n",
    "tmp_fins = tmp.copy()[tmp.fins==1]\n",
    "# Sum over total per exporter, importer combo\n",
    "tmp_sum = tmp[[\"ISOex_i\",\"ISOim_j\",\"quantity_q\"]].groupby([\"ISOex_i\",\"ISOim_j\"]).sum()\n",
    "# Sum fins over total per year, exporter, importer\n",
    "tmp_fins_sum = tmp_fins[[\"ISOex_i\",\"ISOim_j\",\"quantity_q\"]].groupby([\"ISOex_i\",\"ISOim_j\"]).sum()\n",
    "# Grab proportion of fins where there are any\n",
    "tmp_fin_props = (tmp_fins_sum/tmp_sum).reset_index()\n",
    "# Make zero fins where there are none\n",
    "maskx = tmp_fin_props.quantity_q.isna()\n",
    "tmp_fin_props.loc[maskx,'quantity_q'] = 0\n",
    "# Convert to proportions of meat\n",
    "tmp_meat_props = tmp_fin_props.copy()\n",
    "tmp_meat_props['prop_sharks'] = 1-tmp_meat_props.quantity_q\n",
    "tmp_meat_props = tmp_meat_props.drop(columns=['quantity_q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata_old = tdata.copy()\n",
    "# Iterate over exporter/importer combos\n",
    "for i in range(tmp_meat_props.shape[0]):\n",
    "    exi = tmp_meat_props.ISOex_i[i]\n",
    "    imi = tmp_meat_props.ISOim_j[i]\n",
    "    shark_prop = tmp_meat_props.prop_sharks[i]\n",
    "    # Re-scale observed sharks pre 2017 to remove fins fraction from fresh or frozen sharks\n",
    "    maskx = ((tdata.year_t<2017) & (tdata.ISOex_i==exi) & (tdata.ISOim_j==imi)&(tdata.group=='sharks'))\n",
    "    tdata.loc[maskx,'quantity_q'] = tdata.loc[maskx,'quantity_q']*shark_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(tdata_old.quantity_q,tdata.quantity_q)\n",
    "#plt.xlabel('All shark trade')\n",
    "#plt.ylabel('Adjusted shark trade')\n",
    "#plt.title('Shark trade with fins removed 2012-2017');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to live weights\n",
    "tdata['estimated_live_weight'] = tdata.quantity_q\n",
    "# Sharks conversion\n",
    "tmpmask = tdata.group=='sharks'\n",
    "tdata.loc[tmpmask,'estimated_live_weight'] = tdata.loc[tmpmask,'estimated_live_weight']*2\n",
    "# Rays conversion\n",
    "tmpmask = tdata.group=='rays'\n",
    "tdata.loc[tmpmask,'estimated_live_weight'] = tdata.loc[tmpmask,'estimated_live_weight']*1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reset biggest_countries to include only those with landings\n",
    "biggest_countries = country_\n",
    "\n",
    "# - - - - - - - - - - - Add BACI total seafood trade\n",
    "total_seafood_trade = (\n",
    "    odata[\n",
    "        ((odata.ISOex_i).isin(biggest_countries))\n",
    "        & ((odata.ISOim_j).isin(biggest_countries))\n",
    "    ]\n",
    "    .groupby([\"ISOex_i\", \"ISOim_j\"])\n",
    "    .sum()[\"ij_total\"]\n",
    "    .reset_index()\n",
    "    .set_index(\"ISOex_i\")\n",
    "    .pivot(columns=\"ISOim_j\")\n",
    "    .droplevel(0, axis=\"columns\")\n",
    "    .fillna(0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reset biggest_countries to include only those with landings\n",
    "biggest_countries = country_\n",
    "\n",
    "# - - - - - - - - - - - Add BACI total seafood trade\n",
    "total_seafood_trade = (\n",
    "    odata[\n",
    "        ((odata.ISOex_i).isin(biggest_countries))\n",
    "        & ((odata.ISOim_j).isin(biggest_countries))\n",
    "    ]\n",
    "    .groupby([\"ISOex_i\", \"ISOim_j\"])\n",
    "    .sum()[\"ij_total\"]\n",
    "    .reset_index()\n",
    "    .set_index(\"ISOex_i\")\n",
    "    .pivot(columns=\"ISOim_j\")\n",
    "    .droplevel(0, axis=\"columns\")\n",
    "    .fillna(0.0)\n",
    ")\n",
    "\n",
    "# Better country labels\n",
    "biggest_countries_long = kdata.country_name_abbreviation[\n",
    "    [list(kdata.iso_3digit_alpha).index(x) for x in biggest_countries]\n",
    "].to_numpy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with re-exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # REEXPORTS - NEED TO CHECK THAT RAYS ARE COOL HERE TOO\n",
    "#\n",
    "# Currently removes trade that has no possible catch.\n",
    "#\n",
    "# NB:\n",
    "#\n",
    "# 1. Assumes catches in year t are traded in year t\n",
    "\n",
    "# Pre-removals copy\n",
    "tdata_copy = tdata.copy()\n",
    "# Unique country list\n",
    "tmp_c = np.unique(np.array(list(tdata.ISOex_i)+list(tdata.ISOim_j)))\n",
    "# Unique commodity codes\n",
    "tmp_u = tdata.group.unique()\n",
    "# Landings per country per year per commodity\n",
    "tmp_l = fdata.groupby([\"country_code\",\"year\",\"group\"]).sum().reset_index().sort_values(\"landed_weight\", ascending=False)\n",
    "\n",
    "\n",
    "# Empty list of identified re-exports\n",
    "tmp = []\n",
    "\n",
    "# ====================== Remove re-exports from trade =========================== #\n",
    "# Iterate over years\n",
    "for y in tdata.year_t.unique():\n",
    "    # Grab values for year y \n",
    "    trad_ = tdata[tdata.year_t==y].groupby([\"ISOex_i\",'year_t','group']).sum().reset_index().sort_values(\"estimated_live_weight\", ascending=False)\n",
    "    # Grab total trade for year y\n",
    "    trad = trad_.groupby([\"ISOex_i\",'year_t']).sum().reset_index().sort_values(\"estimated_live_weight\", ascending=False)\n",
    "    # Grab group trade for year y\n",
    "    trad_s = trad_[trad_.group=='sharks']\n",
    "    trad_r = trad_[trad_.group=='rays']\n",
    "    # Grab total landings for year y\n",
    "    land = tmp_l[tmp_l.year==y]\n",
    "    # Grab possible group landings for year y\n",
    "    land_s = land[land.group=='sharks']\n",
    "    land_r = land[land.group=='rays']\n",
    "    land_e = land[land.group=='elasmos']\n",
    "    # Iteratre over countries\n",
    "    for e in tmp_c:\n",
    "        if e in land.country_code.unique() and e in trad.ISOex_i.unique():\n",
    "            # Grab values for exporter e in year y\n",
    "            tx = trad[trad.ISOex_i==e]\n",
    "            tx_s = trad_s[trad_s.ISOex_i==e]\n",
    "            tx_r = trad_r[trad_r.ISOex_i==e]\n",
    "            # Grab landings\n",
    "            lx = land[land.country_code==e]\n",
    "            lx_s = land_s[land_s.country_code==e]\n",
    "            lx_r = land_r[land_r.country_code==e]\n",
    "            lx_e = land_e[land_e.country_code==e]\n",
    "\n",
    "            # If no catches to support trade, make trade zero to remove re-exports\n",
    "            # Do this first because no catches of any kind trump group specifics\n",
    "            # justified because the project is about assigning catches within the trade to specific nations\n",
    "            if sum(lx.landed_weight)==0 and sum(tx.estimated_live_weight)>0:\n",
    "                tmp += [sum(tx.estimated_live_weight)]\n",
    "                maskx = (tdata.ISOex_i==e) & (tdata.year_t==y)\n",
    "                tdata.loc[maskx,'estimated_live_weight'] = 0\n",
    "                \n",
    "            # If no catches (group+elasmos) to support shark trade, make trade zero to remove re-exports: \n",
    "            if sum(lx_s.landed_weight+lx_e.landed_weight)==0 and sum(tx_s.estimated_live_weight)>0:\n",
    "                tmp += [sum(tx_s.estimated_live_weight)]\n",
    "                maskx = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='sharks')\n",
    "                tdata.loc[maskx,'estimated_live_weight'] = 0\n",
    "                \n",
    "            # If no catches (group+elasmos) to support ray trade, make trade zero to remove re-exports:\n",
    "            if sum(lx_r.landed_weight+lx_e.landed_weight)==0 and sum(tx_r.estimated_live_weight)>0:\n",
    "                tmp += [sum(tx_r.estimated_live_weight)]\n",
    "                maskx = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='rays')\n",
    "                tdata.loc[maskx,'estimated_live_weight'] = 0\n",
    "                \n",
    "            # If trade more than catches, make proportional within allowable group\n",
    "            elif sum(lx.landed_weight)<sum(tx.estimated_live_weight):\n",
    "                tmp += [sum(tx.estimated_live_weight)]\n",
    "                # Grab proportion\n",
    "                rrx = sum(lx.landed_weight)/sum(tx.estimated_live_weight)\n",
    "                # Re-scale trade to proportion of total landings\n",
    "                mask_s = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='sharks')\n",
    "                tdata.loc[mask_s,'estimated_live_weight'] = tdata.loc[mask_s,'estimated_live_weight']*rrx\n",
    "                mask_r = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='rays')\n",
    "                tdata.loc[mask_r,'estimated_live_weight'] = tdata.loc[mask_r,'estimated_live_weight']*rrx\n",
    "        else:\n",
    "            mask = ((tdata.ISOex_i==e) & (tdata.year_t==y))\n",
    "            tdata.loc[mask,'estimated_live_weight'] = 0\n",
    "            #print(e,y)        \n",
    "\n",
    "# ## Restrict landings, seafood trade and meat trade to biggest countries\n",
    "\n",
    "# Original\n",
    "tmp_x = (tdata_copy.estimated_live_weight)\n",
    "# Updated\n",
    "tmp_y = (tdata.estimated_live_weight)\n",
    "# Countries with trade reduced\n",
    "iredux = (tdata_copy.estimated_live_weight-tdata.estimated_live_weight)>1\n",
    "# Grap re-exports data\n",
    "ReExports = pd.DataFrame(zip(tmp_x[iredux],tmp_y[iredux],tdata.ISOex_i[iredux].to_numpy(),tdata_copy.year_t[iredux],tdata_copy.group[iredux]),columns=['Original','Reduced','exporter','year','group'])\n",
    "ReExports['Net_diff'] = ReExports.Original-ReExports.Reduced\n",
    "ReExports['Exporter'] = kdata.country_name_abbreviation[[list(kdata.iso_3digit_alpha).index(x) for x in ReExports.exporter]].to_numpy()\n",
    "\n",
    "# Table of re-exporting countries\n",
    "tmp = ReExports.groupby(['Exporter']).sum().sort_values(by='Net_diff',ascending=False).drop(columns='exporter')\n",
    "tmp.to_csv('ReExport_totals_AUGM.csv')\n",
    "# Local augmented re-exports\n",
    "ReExports_AUGM = ReExports\n",
    "\n",
    "# = = = = = = = = = = = = = #\n",
    "# Remove zeros from trade\n",
    "tdata = tdata[tdata.estimated_live_weight!=0]\n",
    "\n",
    "# = = = = = = = = = = = = = #\n",
    "# Remove fins\n",
    "tdata = tdata[tdata['product']=='meat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check FAO trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FAO trade data\n",
    "rdata = pd.read_csv(bd + \"FAO_elasmo.csv\")\n",
    "# Drop rows with zero trade\n",
    "rdata = rdata[rdata.trade_quantity>0]\n",
    "# Drop rows outside of 2012-2019\n",
    "rdata = rdata[rdata.year<=2019]\n",
    "# Drop NA\n",
    "rdata = rdata[rdata.trade_quantity.notna()]\n",
    "rdata = rdata[rdata.commodity_fao.notna()]\n",
    "\n",
    "# Drop other trade\n",
    "rdata = rdata[rdata.commodity_fao!='Shark fins, smoked, dried, whether or not salted, etc.']\n",
    "rdata = rdata[rdata.commodity_fao!='Shark fins, prepared or preserved']\n",
    "rdata = rdata[rdata.commodity_fao!='Shark fins, frozen']\n",
    "rdata = rdata[rdata.commodity_fao!='Shark fins, salted and in brine but not dried or smoked']\n",
    "rdata = rdata[rdata.commodity_fao!='Shark fins, dried, unsalted']\n",
    "rdata = rdata[rdata.commodity_fao!='Sharks, dried, whether or not salted, but not smoked']\n",
    "rdata = rdata[rdata.commodity_fao!='Shark oil']\n",
    "rdata = rdata[rdata.commodity_fao!='Shark liver oil']\n",
    "rdata = rdata[rdata.commodity_fao!='Sharks, fillets, dried, salted or in brine']\n",
    "\n",
    "# Drop countries outside of top traders\n",
    "tindx = (match(biggest_countries_long,list(rdata.reporting_country.values))[\n",
    "         match(biggest_countries_long,list(rdata.reporting_country.values))!=None]\n",
    "        )\n",
    "rdata = rdata.iloc[tindx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab codes\n",
    "FAOcodes = pd.DataFrame({'code':rdata.commodity_fao.unique()})\n",
    "FAOcodes['group'] = np.array(['rays','sharks'])[FAOcodes.code.str.contains('shark').values*1]\n",
    "FAOcodes.group[(FAOcodes.code.str.contains('shark',case=False, regex=False, na=False).values*1\n",
    "                +FAOcodes.code.str.contains('ray',case=False, regex=False, na=False).values*1)==2] = 'both'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write external key\n",
    "pd.DataFrame(columns=FAOcodes.code,index=srkey.species_binomial).to_csv('FAO_elasmo_key.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>reporting_country</th>\n",
       "      <th>commodity_fao</th>\n",
       "      <th>trade_flow</th>\n",
       "      <th>unit</th>\n",
       "      <th>year</th>\n",
       "      <th>trade_quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>Argentina</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>245.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>190</th>\n",
       "      <td>Belgium</td>\n",
       "      <td>Catsharks, nursehounds, fresh or chilled</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>178.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>277</th>\n",
       "      <td>Brazil</td>\n",
       "      <td>Blue shark \"Prionace glauca\", fillets, frozen</td>\n",
       "      <td>Imports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>1731.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>365</th>\n",
       "      <td>Canada</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>301.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>420</th>\n",
       "      <td>China</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>1920.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>487</th>\n",
       "      <td>Colombia</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Imports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>241.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>507</th>\n",
       "      <td>Costa Rica</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>1863.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>734</th>\n",
       "      <td>Ecuador</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>67.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>861</th>\n",
       "      <td>France</td>\n",
       "      <td>Catsharks, nursehounds, fresh or chilled</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>252.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1189</th>\n",
       "      <td>Indonesia</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>947.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1277</th>\n",
       "      <td>Italy</td>\n",
       "      <td>Catsharks, nursehounds, fresh or chilled</td>\n",
       "      <td>Imports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>149.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1459</th>\n",
       "      <td>Libya</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Imports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>239.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13221</th>\n",
       "      <td>Mauritania</td>\n",
       "      <td>Sharks nei, fresh or chilled</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2016</td>\n",
       "      <td>1410.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1704</th>\n",
       "      <td>Namibia</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>1297.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1864</th>\n",
       "      <td>Nigeria</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Imports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>126.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1806</th>\n",
       "      <td>New Zealand</td>\n",
       "      <td>Dogfish (Squalidae) and catshark fillets, frozen</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>310.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1896</th>\n",
       "      <td>Oman</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>350.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7722</th>\n",
       "      <td>Pakistan</td>\n",
       "      <td>Rays and skates (Rajidae), frozen</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2014</td>\n",
       "      <td>55.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1926</th>\n",
       "      <td>Panama</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>219.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1959</th>\n",
       "      <td>Peru</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>1197.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1976</th>\n",
       "      <td>Philippines</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Imports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>114.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2269</th>\n",
       "      <td>Singapore</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>1434.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2586</th>\n",
       "      <td>Thailand</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>488.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2623</th>\n",
       "      <td>Trinidad and Tobago</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>98.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2825</th>\n",
       "      <td>Uruguay</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>9412.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2867</th>\n",
       "      <td>Viet Nam</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>2591.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2846</th>\n",
       "      <td>Vanuatu</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>63.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2885</th>\n",
       "      <td>Yemen</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2376</th>\n",
       "      <td>South Africa</td>\n",
       "      <td>Dogfish and other sharks nei (excl. Squalus ac...</td>\n",
       "      <td>Exports</td>\n",
       "      <td>Tonnes – net product weight</td>\n",
       "      <td>2012</td>\n",
       "      <td>674.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         reporting_country                                      commodity_fao  \\\n",
       "41               Argentina  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "190                Belgium           Catsharks, nursehounds, fresh or chilled   \n",
       "277                 Brazil      Blue shark \"Prionace glauca\", fillets, frozen   \n",
       "365                 Canada  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "420                  China  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "487               Colombia  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "507             Costa Rica  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "734                Ecuador  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "861                 France           Catsharks, nursehounds, fresh or chilled   \n",
       "1189             Indonesia  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "1277                 Italy           Catsharks, nursehounds, fresh or chilled   \n",
       "1459                 Libya  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "13221           Mauritania                       Sharks nei, fresh or chilled   \n",
       "1704               Namibia  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "1864               Nigeria  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "1806           New Zealand   Dogfish (Squalidae) and catshark fillets, frozen   \n",
       "1896                  Oman  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "7722              Pakistan                  Rays and skates (Rajidae), frozen   \n",
       "1926                Panama  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "1959                  Peru  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "1976           Philippines  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2269             Singapore  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2586              Thailand  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2623   Trinidad and Tobago  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2825               Uruguay  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2867              Viet Nam  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2846               Vanuatu  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2885                 Yemen  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "2376          South Africa  Dogfish and other sharks nei (excl. Squalus ac...   \n",
       "\n",
       "      trade_flow                         unit  year  trade_quantity  \n",
       "41       Exports  Tonnes – net product weight  2012           245.0  \n",
       "190      Exports  Tonnes – net product weight  2012           178.0  \n",
       "277      Imports  Tonnes – net product weight  2012          1731.0  \n",
       "365      Exports  Tonnes – net product weight  2012           301.0  \n",
       "420      Exports  Tonnes – net product weight  2012          1920.0  \n",
       "487      Imports  Tonnes – net product weight  2012           241.0  \n",
       "507      Exports  Tonnes – net product weight  2012          1863.0  \n",
       "734      Exports  Tonnes – net product weight  2012            67.0  \n",
       "861      Exports  Tonnes – net product weight  2012           252.0  \n",
       "1189     Exports  Tonnes – net product weight  2012           947.0  \n",
       "1277     Imports  Tonnes – net product weight  2012           149.0  \n",
       "1459     Imports  Tonnes – net product weight  2012           239.0  \n",
       "13221    Exports  Tonnes – net product weight  2016          1410.0  \n",
       "1704     Exports  Tonnes – net product weight  2012          1297.0  \n",
       "1864     Imports  Tonnes – net product weight  2012           126.0  \n",
       "1806     Exports  Tonnes – net product weight  2012           310.0  \n",
       "1896     Exports  Tonnes – net product weight  2012           350.0  \n",
       "7722     Exports  Tonnes – net product weight  2014            55.0  \n",
       "1926     Exports  Tonnes – net product weight  2012           219.0  \n",
       "1959     Exports  Tonnes – net product weight  2012          1197.0  \n",
       "1976     Imports  Tonnes – net product weight  2012           114.0  \n",
       "2269     Exports  Tonnes – net product weight  2012          1434.0  \n",
       "2586     Exports  Tonnes – net product weight  2012           488.0  \n",
       "2623     Exports  Tonnes – net product weight  2012            98.0  \n",
       "2825     Exports  Tonnes – net product weight  2012          9412.0  \n",
       "2867     Exports  Tonnes – net product weight  2012          2591.0  \n",
       "2846     Exports  Tonnes – net product weight  2012            63.0  \n",
       "2885     Exports  Tonnes – net product weight  2012           193.0  \n",
       "2376     Exports  Tonnes – net product weight  2012           674.0  "
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rdata[rdata.trade_quantity>50]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup landings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out shark and ray trade\n",
    "tdata_cut = tdata[\n",
    "    ((tdata.ISOex_i).isin(biggest_countries))\n",
    "    & ((tdata.ISOim_j).isin(biggest_countries))\n",
    "]\n",
    "# dropping missing values\n",
    "tdata_cut = tdata_cut.drop(\"Unnamed: 0\", axis=\"columns\").dropna().reset_index(drop=True)\n",
    "# Summarize data by importer export commodity\n",
    "tdata_cut = tdata_cut.groupby([\"ISOex_i\", \"ISOim_j\", \"group\",'year_t']).sum().reset_index()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get average per year\n",
    "tdata_cut = (tdata_cut[[\"ISOex_i\", \"ISOim_j\", \"group\",\"year_t\",'quantity_q','estimated_live_weight']]\n",
    "             .groupby([\"ISOex_i\", \"ISOim_j\", \"group\"])\n",
    "             .mean()\n",
    "             .drop(columns=['year_t'])\n",
    "             .reset_index()\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we just care about whether it's a shark or ray, not if it's also fresh or frozen\n",
    "tdata_cut = tdata_cut.sort_values([\"ISOex_i\", \"ISOim_j\", \"group\"])\n",
    "\n",
    "# Shark data\n",
    "tdata_cut_sharks = (\n",
    "    tdata_cut[tdata_cut.group == \"sharks\"]\n",
    "    .groupby([\"ISOex_i\", \"ISOim_j\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ray data\n",
    "tdata_cut_rays = (\n",
    "    tdata_cut[tdata_cut.group == \"rays\"]\n",
    "    .groupby([\"ISOex_i\", \"ISOim_j\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Load unreliability score\n",
    "\n",
    "unreliability_score = pd.read_csv(\n",
    "    bd + \"reporter_reliability_HS12_V202102.csv\",\n",
    "    usecols=[\"c\", \"q_unreliability_i\", \"q_unreliability_j\"],\n",
    ")\n",
    "\n",
    "# ITA changed code -- because, why not?\n",
    "unreliability_score.loc[unreliability_score.c == 381, \"c\"] = 380\n",
    "\n",
    "# grab ISO codes from odata\n",
    "unreliability_score = unreliability_score.rename(\n",
    "    columns={\"q_unreliability_i\": \"exporter\", \"q_unreliability_j\": \"importer\"}\n",
    ").merge(odata, left_on=\"c\", right_on=\"exporter_i\")\n",
    "unreliability_score = (\n",
    "    unreliability_score[\n",
    "        ((unreliability_score.ISOex_i).isin(biggest_countries))\n",
    "        & ((unreliability_score.ISOim_j).isin(biggest_countries))\n",
    "    ][[\"ISOex_i\", \"exporter\", \"importer\"]]\n",
    "    .groupby(\"ISOex_i\")\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Check for missing unreliability scores\n",
    "misx = biggest_countries[\n",
    "    np.array([x not in unreliability_score.index for x in biggest_countries])\n",
    "]\n",
    "# Fill missing unreliablity scores with maximum unreliability value\n",
    "tmp_maxval = max(unreliability_score.exporter)\n",
    "if len(misx) > 0:\n",
    "    for i in misx:\n",
    "        unreliability_score = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame({'exporter': [0], 'importer': [0]}, index=[i]),\n",
    "                unreliability_score,\n",
    "            ]\n",
    "        )\n",
    "\n",
    "unreliability_score = unreliability_score.sort_index().fillna(tmp_maxval)\n",
    "unreliability_score = unreliability_score.reindex(\n",
    "    sorted(unreliability_score.columns), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup taxon masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count taxon aggregations\n",
    "ntax_country = (\n",
    "    txdata.groupby(by=([\"country\", \"taxon\"]))\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .groupby(\"country\")\n",
    "    .count()\n",
    "    .taxon\n",
    ")\n",
    "\n",
    "# Add Belize\n",
    "ntax_country[\"BLZ\"] = 0\n",
    "# Re-order to match country_\n",
    "ntax_country = ntax_country[country_]\n",
    "\n",
    "# Taxon by country groupings\n",
    "taxindx1 = (ntax_country <= 1).to_numpy()\n",
    "taxindx2 = (ntax_country == 2).to_numpy()\n",
    "taxindx3 = (ntax_country >= 3).to_numpy()\n",
    "\n",
    "# Create 3 dimensional mask\n",
    "TaxonMASK_t1 = TaxonMASK_Sx.copy()\n",
    "TaxonMASK_t2 = TaxonMASK_Sx.copy()\n",
    "TaxonMASK_t3 = TaxonMASK_Sx.copy()\n",
    "\n",
    "# Deactivate countries without <=1, 2, or >=3 taxon groups reported\n",
    "TaxonMASK_t1[taxindx1 == False, ...] = 0\n",
    "TaxonMASK_t2[taxindx2 == False, ...] = 0\n",
    "TaxonMASK_t3[taxindx3 == False, ...] = 0\n",
    "\n",
    "# Make sure Elasmos bin is positive in countries with no aggregations\n",
    "NoTaxAgg = (NoTaxaSppWT.sum(1)==0)*1\n",
    "TaxonMASK_Sx[NoTaxAgg!=1,:,list(taxon_shortlist).index('Elasmobranchii')] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `dims` and `coords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some countries can be missing from importers or exporters\n",
    "# indexing needs to take that into account\n",
    "# if we used `factorize`, that would be ignored\n",
    "\n",
    "country_to_idx_map = {country: index for index, country in enumerate(biggest_countries)}\n",
    "shark_exporter_idx = tdata_cut_sharks[\"ISOex_i\"].map(country_to_idx_map).to_numpy()\n",
    "shark_importer_idx = tdata_cut_sharks[\"ISOim_j\"].map(country_to_idx_map).to_numpy()\n",
    "ray_exporter_idx = tdata_cut_rays[\"ISOex_i\"].map(country_to_idx_map).to_numpy()\n",
    "ray_importer_idx = tdata_cut_rays[\"ISOim_j\"].map(country_to_idx_map).to_numpy()\n",
    "\n",
    "# You have to be careful when creating shark_trade_matrix:\n",
    "shark_trade_matrix = (\n",
    "    tdata_cut_sharks[[\"ISOex_i\", \"ISOim_j\", \"estimated_live_weight\"]]\n",
    "    .set_index(\"ISOex_i\")\n",
    "    .pivot(columns=\"ISOim_j\")\n",
    "    .droplevel(0, axis=\"columns\")\n",
    ")\n",
    "\n",
    "# Add missing exporters and importers\n",
    "missing_col = []\n",
    "for p in country_:\n",
    "    if not p in shark_trade_matrix.columns.values:\n",
    "        missing_col.append(p)\n",
    "missing_col = np.array(missing_col)\n",
    "shark_trade_matrix[missing_col] = np.NaN\n",
    "shark_trade_matrix = shark_trade_matrix[country_]\n",
    "missing_row = shark_trade_matrix.columns.difference(shark_trade_matrix.index)\n",
    "shark_trade_matrix = shark_trade_matrix.T\n",
    "shark_trade_matrix[missing_row] = np.NaN\n",
    "shark_trade_matrix = shark_trade_matrix[country_]\n",
    "shark_trade_matrix = shark_trade_matrix.T.sort_index().fillna(0)\n",
    "shark_trade_mask = shark_trade_matrix.to_numpy()\n",
    "shark_trade_mask[shark_trade_mask>0] = 1\n",
    "# Add domestic consumption\n",
    "np.fill_diagonal(shark_trade_mask,1)\n",
    "\n",
    "# You have to be careful when creating ray_trade_matrix:\n",
    "ray_trade_matrix = (\n",
    "    tdata_cut_rays[[\"ISOex_i\", \"ISOim_j\", \"estimated_live_weight\"]]\n",
    "    .set_index(\"ISOex_i\")\n",
    "    .pivot(columns=\"ISOim_j\")\n",
    "    .droplevel(0, axis=\"columns\")\n",
    ")\n",
    "\n",
    "# Add missing exporters and importers\n",
    "#missing_col = ray_trade_matrix.index.difference(ray_trade_matrix.columns)\n",
    "missing_col = []\n",
    "for p in country_:\n",
    "    if not p in ray_trade_matrix.columns.values:\n",
    "        missing_col.append(p)\n",
    "missing_col = np.array(missing_col)\n",
    "ray_trade_matrix[missing_col] = np.NaN\n",
    "ray_trade_matrix = ray_trade_matrix[country_]\n",
    "missing_row = ray_trade_matrix.columns.difference(ray_trade_matrix.index)\n",
    "ray_trade_matrix = ray_trade_matrix.T\n",
    "ray_trade_matrix[missing_row] = np.NaN\n",
    "ray_trade_matrix = ray_trade_matrix[country_]\n",
    "ray_trade_matrix = ray_trade_matrix.T.sort_index().fillna(0)\n",
    "ray_trade_mask = ray_trade_matrix.to_numpy()\n",
    "ray_trade_mask[ray_trade_mask>0] = 1\n",
    "# Add domestic consumption\n",
    "np.fill_diagonal(ray_trade_mask,1)\n",
    "\n",
    "# Species mask for possible trade (including domestic)\n",
    "trade_mask = ray_trade_mask[:,None,:]*((group_=='rays')[None,:,None])+shark_trade_mask[:,None,:]*((group_=='sharks')[None,:,None])\n",
    "trade_mask[trade_mask==0] = -9\n",
    "trade_mask[trade_mask>0] = 0\n",
    "# Remove species not used for meat\n",
    "#trade_mask = trade_mask+meat_mask[None,:,None]\n",
    "trade_mask[trade_mask<0] = -9\n",
    "\n",
    "# Mask for trade softmax to zero out species with all -9\n",
    "NoSPP_Mask = (((trade_mask==-9).sum(2)!=len(country_))*1)\n",
    "\n",
    "# Mask for blue shark relative odds importer preferences\n",
    "BSmask = np.zeros(shape=trade_mask[0].shape)\n",
    "BSmask[list(species_).index('Prionace glauca')] = -9\n",
    "\n",
    "# Better country labels\n",
    "biggest_countries_long = kdata.country_name_abbreviation[\n",
    "    [list(kdata.iso_3digit_alpha).index(x) for x in biggest_countries]\n",
    "].to_numpy()\n",
    "\n",
    "# Create matching tensor for priors\n",
    "SppPRIORadj_idx = SppPRIORadj.copy()\n",
    "# List of unique prior values\n",
    "priors_ = list(np.sort(np.unique(SppPRIORadj)))\n",
    "# Replace prior values with index to OddsCAT\n",
    "for i in range(len(SppPRIORadj_idx)):\n",
    "    SppPRIORadj_idx[i] = match(SppPRIORadj_idx[i],priors_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Observed data country index \n",
    "obs_exporter_idx = np.array([country_to_idx_map[x] for x in Obscou])\n",
    "\n",
    "# Tmp fix of priors set too low for observed species\n",
    "tmp = ((SppPRIOR[obs_exporter_idx,:]==-999) & (ObsLandings.values>0))\n",
    "\n",
    "for t in range(len(tmp.sum(1))):\n",
    "    if tmp[t].sum()>0:\n",
    "        tmpc = ObsLandings.country[t].values\n",
    "        tmps = ObsLandings.species[tmp[t]].values\n",
    "        tmpo = ObsLandings.sel(country=tmpc,species=tmps).values\n",
    "        tmpm = priorImportance.sel(country=tmpc,species=tmps).values\n",
    "        #print(tmpc,tmps,tmpo,tmpm)\n",
    "        # tmp fix of priors\n",
    "        SppPRIOR[country_==tmpc,np.isin(species_,tmps)] = 1\n",
    "        SppPRIORadj[country_==tmpc,np.isin(species_,tmps)] = 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Trade weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace values in revision tradeweights with tradeIportance priors\n",
    "for e in country_:\n",
    "    for s in species_:\n",
    "        tradeweights.values[country_==c,species_==s,country_!=c] = tradeImportance.sel(country=e,species=s).values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace -999 with -9\n",
    "tradeweights.values[tradeweights==-999] = -9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab bilateral trade weights\n",
    "bdata = pd.read_csv(bd + \"bilateral_trade.csv\").copy()\n",
    "\n",
    "# Update specific weights\n",
    "for r in bdata.itertuples():\n",
    "    tradeweights.values[country_==r.exporter,species_==r.species,country_==r.importer] = r.tradeweight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "COORDS = {\n",
    "    \"exporter\": biggest_countries,\n",
    "    \"importer\": biggest_countries,\n",
    "    \"obs_exporter\": country_[obs_exporter_idx],\n",
    "    \"shark_obs_idx\": tdata_cut_sharks.index,\n",
    "    \"ray_obs_idx\": tdata_cut_rays.index,\n",
    "    \"direction\": [\"exports\", \"imports\"],\n",
    "    \"quantity\": [\"weight\", \"value\"],\n",
    "    \"species\": species_,\n",
    "    \"landing_country\": country_,\n",
    "    \"taxon\": taxon_shortlist,\n",
    "    \"year\":year_,\n",
    "    \"OddsCAT\":np.unique(SppPRIORadj).astype(str)\n",
    "}\n",
    "print(\"Data loaded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
