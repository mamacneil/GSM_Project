{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model plots for shark and ray meat landings and trade applied to 2012-2019 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "# # Model for shark and ray meat landings and trade applied to 2014-2019 data\n",
    "\n",
    "# In[1]:\n",
    "\n",
    "\n",
    "import os\n",
    "import pdb\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib as mp\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pyt\n",
    "import rdata as rd\n",
    "import seaborn as sns\n",
    "import xarray as xr\n",
    "import scipy as sp\n",
    "from matplotlib.gridspec import GridSpec\n",
    "\n",
    "# Set figure style.\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "bd = os.getcwd() + \"/../Data/\"\n",
    "\n",
    "# Helper functions\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo, Ix\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "match = lambda a, b: np.array([b.index(x) if x in b else None for x in a])\n",
    "\n",
    "\n",
    "def unique(series: pd.Series):\n",
    "    \"Helper function to sort and isolate unique values of a Pandas Series\"\n",
    "    return series.sort_values().unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load landings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In[83]:\n",
    "\n",
    "#'''\n",
    "#dnam = bd + \"modeldatsharksrays_land_trade_2012-2019\"\n",
    "dnam = bd + \"modeldatsharksrays_land_trade_240828\"\n",
    "\n",
    "parsed = rd.parser.parse_file(dnam + \".RData\")\n",
    "converted = rd.conversion.convert(parsed)\n",
    "tmp = converted[\"modeldat\"]\n",
    "#'''\n",
    "\n",
    "#'''\n",
    "# Matrices\n",
    "speciesCountryIDMap = (\n",
    "    tmp[\"speciesCountryIDMap\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\", \"dim_2\": \"taxon\"})\n",
    "    .transpose(\"country\", \"species\", \"taxon\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "logProbPrior = (\n",
    "    tmp[\"logProbPrior\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\", \"dim_2\": \"taxon\"})\n",
    "    .transpose(\"country\", \"species\", \"taxon\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "priorImportance = (\n",
    "    tmp[\"priorImportance\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "OutputSpeciesCountryMapFull = (\n",
    "    tmp[\"OutputSpeciesCountryMapFull\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "OutputSpeciesCountryMap = (\n",
    "    tmp[\"OutputSpeciesCountryMap\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "SpeciesCommodityMap = (\n",
    "    tmp[\"speciesCommodities\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"commodity\"})\n",
    "    .sortby(\"species\")\n",
    ")\n",
    "\n",
    "# Grab key for shark/ray\n",
    "srkey = pd.read_csv(bd + \"taxonomy_20240205.csv\")\n",
    "srkey['group'] = srkey.Superorder.replace('Batoidea','rays').replace('Selachimorpha','sharks').to_numpy()\n",
    "\n",
    "# Trade covariate\n",
    "tradeImportance = (\n",
    "    tmp[\"tradeImportance\"]\n",
    "    .rename({\"dim_0\": \"species\", \"dim_1\": \"country\"})\n",
    "    .transpose(\"country\", \"species\")\n",
    "    .sortby(\"country\")\n",
    ")\n",
    "\n",
    "# Observed landings\n",
    "ObsLandings_ = pd.melt(tmp[\"national_landings_info\"], id_vars='country', value_name='landings')\n",
    "ObsLandings_.columns = ['country','species','obs_landings']\n",
    "ObsLandings_.species = np.array([s.replace('.' , ' ') for s in ObsLandings_.species.values])\n",
    "ObsLandings_ = ObsLandings_.set_index(['country','species']).to_xarray().to_dataarray()[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensure available taxon matches for reported species"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#\"\"\"\n",
    "# Initialize species to taxon mapping\n",
    "SpeciesTaxonMAP = speciesCountryIDMap[0].drop_vars('country')\n",
    "\n",
    "# Match taxa to species level regardless of taxonomic level of aggregation\n",
    "for t in speciesCountryIDMap.taxon.values:\n",
    "    # Iterate over possible species for each taxon\n",
    "    for s in srkey[srkey.isin([t]).any(axis=1)]['species_binomial'].values:\n",
    "        try:\n",
    "            SpeciesTaxonMAP.loc[dict(species=s,taxon=t)] = 1\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "# List of rarely caught species \n",
    "drop_spp = priorImportance.species.to_numpy()[priorImportance.max([\"country\"])<=1]\n",
    "# Number of species remaining with prior importance less than or equal to 1\n",
    "priorImportance.species.shape[0]-drop_spp.shape[0]\n",
    "# Make temporary list of all taxon IDs\n",
    "tmp_taxon_ = speciesCountryIDMap.taxon.to_numpy()\n",
    "# Index taxons relative to what gets landed\n",
    "tmp_TaxonIDx = match(tmp[\"LandingsID\"], list(tmp_taxon_))\n",
    "# Temporary list of all species IDs\n",
    "tmp_species_ = speciesCountryIDMap.species.to_numpy()\n",
    "# Boolean of taxons that are to species level in observed landings\n",
    "tmp_tindx = pd.Series(tmp_taxon_[tmp_TaxonIDx]).str.count(\" \").to_numpy() == 0\n",
    "# Index of species in taxon that are observed as catches\n",
    "tmp_species_spp_id = match(tmp_taxon_[tmp_TaxonIDx[tmp_tindx == 0]], list(tmp_species_))\n",
    "# Unique names of species in taxon that are observed as taxon catches but have prior importance <=1\n",
    "tmp_spp = np.unique(tmp_species_[tmp_species_spp_id[np.log1p(tmp[\"allCatch\"])[tmp_tindx == 0]>0]])\n",
    "# Species in drop list that are actually observed as in taxon+species (taxon) list\n",
    "tmp_spp = list(drop_spp[np.array([x in tmp_spp for x in drop_spp])])\n",
    "\n",
    "# Grab landings data to see which taxons have catch\n",
    "tmp_landings = tmp[\"allCatch\"]\n",
    "tmp_taxon = tmp[\"LandingsID\"]\n",
    "tmp_country = tmp[\"country\"]\n",
    "# Iterate over landings to ensure species are avaiable for taxon in country\n",
    "for l,t,c in zip(tmp_landings,tmp_taxon,tmp_country):\n",
    "    # Look for species landed with impossible priors\n",
    "    try:\n",
    "        if (priorImportance.sel(country=c,species=t)<=-888)*(l>0):\n",
    "            priorImportance.loc[dict(country=c,species=t)] = 2\n",
    "            tmp_spp += [t]\n",
    "            #print(\"Changed \"+c+\" \"+t)\n",
    "    # Look for taxon landed with no possible species \n",
    "    except:\n",
    "        # Possible species for taxon\n",
    "        tax_spp = (SpeciesTaxonMAP.sel(taxon=t).species[SpeciesTaxonMAP.sel(taxon=t)==1]).values\n",
    "        # If all species are impossible for taxon\n",
    "        if (priorImportance.sel(country=c,species=tax_spp).mean()<=-888)*(l>0):\n",
    "            # Assign to possible species in nation\n",
    "            if OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).sum()>0:\n",
    "                axx = tax_spp[OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).to_numpy()>0]\n",
    "                xflax = 'ok'\n",
    "            else:\n",
    "                # Assign to most likely spp given global max\n",
    "                axx = tax_spp[(priorImportance.sel(species=tax_spp).max('country')==priorImportance.sel(species=tax_spp).max())]\n",
    "                xflax = 'not present'\n",
    "            priorImportance.loc[dict(country=c,species=axx)] = 2\n",
    "            tmp_spp += list(axx)\n",
    "            #print(\"Impossible \"+t+\" changed \"+c+\", spp are \"+xflax)\n",
    "            #print(axx)\n",
    "        \n",
    "        # If all species for taxon are below data-reduction cutoff\n",
    "        elif (priorImportance.sel(country=c,species=tax_spp).max()<2)*(l>0):\n",
    "            # Assign to possible species in nation\n",
    "            if OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).sum()>0:\n",
    "                axx = tax_spp[OutputSpeciesCountryMapFull.sel(country=c,species=tax_spp).to_numpy()>0]\n",
    "                xflax = 'ok'\n",
    "            else:\n",
    "                # Assign to most likely spp given global max\n",
    "                axx = tax_spp[(priorImportance.sel(species=tax_spp).max('country')==priorImportance.sel(species=tax_spp).max())]\n",
    "                xflax = 'not present'\n",
    "            priorImportance.loc[dict(country=c,species=axx)] = 2\n",
    "            tmp_spp += list(axx)\n",
    "            #print(\"Low \"+t+\" changed \"+c+\", spp are \"+xflax)\n",
    "            #print(axx)\n",
    "\n",
    "# Add in species in field data\n",
    "field_tmp = np.array(['Atlantoraja cyclhophora', 'Bathtoshia centroura',\n",
    "       'Callorynchus callorynchus',\n",
    "       'Dasyatis hypostigma', 'Fontitrygon geijskesi',\n",
    "       'Hypanus bethalutzae', 'Mobula hypostoma', 'Narcine brasiliensis',\n",
    "       'Pseudobatos horkelii', 'Pteroplatytrygon violacae',\n",
    "       'Rhinoptera brasilisensis', 'Rioraja agassizi',\n",
    "       'Scyliorhinus haekelii', 'Squalus albicaudus', 'Squatina occulta',\n",
    "       'Zapteryx brevirostris'])\n",
    "for s in field_tmp:\n",
    "    if s not in tmp_spp:\n",
    "        tmp_spp+=[s]\n",
    "\n",
    "# Species to drop = have prior importance <=1 AND are not actually observed as taxon to the species level\n",
    "drop_spp = drop_spp[np.array([x not in tmp_spp for x in drop_spp])]\n",
    "\n",
    "#\"\"\"\n",
    "# Drop rare and unreported species\n",
    "speciesCountryIDMap = speciesCountryIDMap.drop_sel(species=drop_spp)\n",
    "priorImportance = priorImportance.drop_sel(species=drop_spp)\n",
    "logProbPrior = logProbPrior.drop_sel(species=drop_spp)\n",
    "SpeciesCommodityMap = SpeciesCommodityMap.drop_sel(species=drop_spp)\n",
    "tradeImportance = tradeImportance.drop_sel(species=drop_spp)\n",
    "\n",
    "# = = = = = = = = = = = = = = After species drop = = = = = = = = = = = = = #\n",
    "\n",
    "# Vectors\n",
    "allCatch = tmp[\"allCatch\"]\n",
    "cindx = allCatch>0\n",
    "allCatch = allCatch[cindx]\n",
    "logCatch = np.log(tmp[\"allCatch\"][cindx])\n",
    "species_ = logProbPrior.species.to_numpy()\n",
    "country_ = logProbPrior.country.to_numpy()\n",
    "CountryIDx = match(tmp[\"country\"][cindx], list(country_))\n",
    "year_ = [\"year_\" + str(x) for x in np.unique(tmp[\"year\"][cindx]).astype(int)]\n",
    "YearIDx = match(tmp[\"year\"][cindx], list(np.unique(tmp[\"year\"][cindx]).astype(int)))\n",
    "taxon_ = logProbPrior.taxon.to_numpy()\n",
    "TaxonIDx = match(tmp[\"LandingsID\"][cindx], list(taxon_))\n",
    "speciesCountryMap = speciesCountryIDMap.groupby(\"species\").max(\"taxon\")\n",
    "TaxonPRIOR = priorImportance.to_numpy()\n",
    "\n",
    "# Meat species\n",
    "#meat_mask = SpeciesCommodityMap.sel(commodity='meat',species=species_).to_numpy()\n",
    "meat_mask = 1*(SpeciesCommodityMap.sel(commodity=('fins'),species=species_)+SpeciesCommodityMap.sel(commodity=('meat'),species=species_)>0).to_numpy()\n",
    "meat_mask[meat_mask==0] = -999\n",
    "meat_mask[meat_mask==1] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match FAO to observed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset observed landings to match FAO countries\n",
    "Obsspp = np.sort(np.array(list(set(ObsLandings_.species.values).intersection(priorImportance.species.values))))\n",
    "Obscou = np.sort(np.array(list(set(ObsLandings_.country.values).intersection(priorImportance.country.values))))\n",
    "ObsLandings = ObsLandings_.sel(country=Obscou, species=Obsspp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tmp empty data to match dimension of full species compliment for observed countries\n",
    "tmpdata = priorImportance.copy()*-0\n",
    "tmpdata.loc[dict(country=ObsLandings.country, species=ObsLandings.species)] = ObsLandings.values\n",
    "ObsLandings = tmpdata.sel(country=Obscou)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average total catch for target of softmax\n",
    "TotalCatch = np.array([allCatch[CountryIDx==i].sum() for i in range(speciesCountryIDMap.shape[0])])/len(year_)\n",
    "logTotalCatch = np.log(TotalCatch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = #\n",
    "\n",
    "# Make group vector\n",
    "group_ = srkey.group[match(species_,list(srkey.species_binomial))].to_numpy()\n",
    "# Make masks for trade sums\n",
    "shark_mask = 1*(group_=='sharks')\n",
    "ray_mask = 1*(group_=='rays')\n",
    "\n",
    "# Grab only aggregated taxa\n",
    "taxon_shortlist = taxon_[pd.Series(taxon_).str.count(\" \").to_numpy() == 0]\n",
    "\n",
    "# Split data index\n",
    "tindx = pd.Series(taxon_[TaxonIDx]).str.count(\" \").to_numpy() == 0\n",
    "# Split landings\n",
    "logReported_species_landings = logCatch[tindx == 0]\n",
    "logReported_taxon_landings = logCatch[tindx == 1]\n",
    "# Split country index\n",
    "country_spp_id = CountryIDx[tindx == 0]\n",
    "country_tax_id = CountryIDx[tindx == 1]\n",
    "# Split year index\n",
    "year_spp_id = YearIDx[tindx == 0]\n",
    "year_tax_id = YearIDx[tindx == 1]\n",
    "# Split species index\n",
    "species_spp_id = match(taxon_[TaxonIDx[tindx == 0]], list(species_))\n",
    "# Split taxon index\n",
    "# taxon_tax_id = TaxonIDx[tindx==1]\n",
    "taxon_tax_id = match(taxon_[TaxonIDx[tindx == 1]], list(taxon_shortlist))\n",
    "\n",
    "\n",
    "\n",
    "# Make dataframes for later\n",
    "sdata = pd.DataFrame(\n",
    "    {\n",
    "        \"logReported_species_landings\": logReported_species_landings,\n",
    "        \"species_spp_id\": species_spp_id,\n",
    "        \"country_spp_id\": country_spp_id,\n",
    "        \"year_spp_id\": year_spp_id,\n",
    "        \"country\": country_[country_spp_id],\n",
    "        \"year\": np.array(year_)[year_spp_id],\n",
    "        \"species\": species_[species_spp_id],\n",
    "    }\n",
    ")\n",
    "txdata = pd.DataFrame(\n",
    "    {\n",
    "        \"logReported_taxon_landings\": logReported_taxon_landings,\n",
    "        \"taxon_tax_id\": taxon_tax_id,\n",
    "        \"country_tax_id\": country_tax_id,\n",
    "        \"year_tax_id\": year_tax_id,\n",
    "        \"country\": country_[country_tax_id],\n",
    "        \"year\": np.array(year_)[year_tax_id],\n",
    "        \"taxon\": taxon_shortlist[taxon_tax_id],\n",
    "    }\n",
    ")\n",
    "txdata = txdata.loc[match(txdata.taxon, list(taxon_shortlist)) != None]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initial mask of species matched to possible taxon\n",
    "InitTaxonMASK = speciesCountryIDMap.copy()\n",
    "# Create empty mask to store observed taxa as possible\n",
    "TaxonMASK = InitTaxonMASK*0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reported taxa by country\n",
    "Obs_tax_data = np.exp(txdata.drop(columns=['year','year_tax_id','country_tax_id','taxon_tax_id']\n",
    "          ).groupby(['country','taxon']).mean()).rename(columns={\"logReported_taxon_landings\": \"Reported_landings\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iteratre over countries\n",
    "for c in country_:\n",
    "    try:\n",
    "        # Iterate over observed taxa and make possible\n",
    "        taxes = Obs_tax_data.loc[c].index.values\n",
    "        for t in taxes:\n",
    "            for s in species_:\n",
    "                TaxonMASK.loc[dict(country=c,species=s,taxon=t)]=InitTaxonMASK.loc[dict(country=c,species=s,taxon=t)]\n",
    "    except:\n",
    "        #print(c)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make priors using relative odds to proportion total landings\n",
    "SppPRIOR = priorImportance.to_numpy()\n",
    "SppPRIORadj = SppPRIOR.copy()\n",
    "# Re-weight to log-odds scale\n",
    "SppPRIORadj[SppPRIORadj==-2]=-4.5\n",
    "SppPRIORadj[SppPRIORadj==-1]=-3.5\n",
    "SppPRIORadj[SppPRIORadj==0]=-2.5\n",
    "SppPRIORadj[SppPRIORadj==1]=-1\n",
    "SppPRIORadj[SppPRIORadj==2]=1\n",
    "SppPRIORadj[SppPRIORadj==3]=4\n",
    "\n",
    "# Cut down taxon MASK to match taxon_shortlist dimensions\n",
    "#TaxonMASK_S = TaxonMASK[:, :, match(taxon_shortlist, list(taxon_))]\n",
    "TaxonMASK_S = TaxonMASK.sel(taxon=taxon_shortlist)\n",
    "TaxonMASK_Sx = TaxonMASK_S.to_numpy()\n",
    "\n",
    "# Negative mask for log-odds zeros\n",
    "negval = -9\n",
    "TaxonMASK_NEG = TaxonMASK_S.copy().to_numpy()\n",
    "TaxonMASK_NEG[TaxonMASK_NEG==0] = negval\n",
    "\n",
    "\n",
    "# Species weight for countries with no aggregations - huge log-odds so p(species ID)=1 where needed\n",
    "NoTaxaSppWT = np.zeros(SppPRIOR.shape)\n",
    "NoTaxaSppWT[(TaxonMASK_NEG != negval).sum(1).sum(1) == 0, :] = abs(negval)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.array(['Carcharhinus acronotus', 'Carcharhinus brevipinna',\n",
    "       'Dasyatis hypostigma', 'Heptranchias perlo',\n",
    "       'Narcine brasiliensis', 'Rhinoptera bonasus',\n",
    "       'Rhizoprionodon lalandii'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Match landings and trade data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make fdata table to merge with trade model\n",
    "fdata = pd.DataFrame(\n",
    "    {\n",
    "        \"year\": YearIDx + 2012,\n",
    "        \"country_code\": country_[CountryIDx],\n",
    "        \"species\": taxon_[TaxonIDx],\n",
    "        \"landed_weight\": allCatch,\n",
    "    }\n",
    ")\n",
    "\n",
    "# Add shark/ray group for each taxon in landings\n",
    "tmp_taxon = fdata.species.unique()\n",
    "tmp_group = []\n",
    "\n",
    "for tx in tmp_taxon:\n",
    "    # taxon at species level\n",
    "    if tx in srkey.species_binomial.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.species_binomial==tx].values[0]]\n",
    "    elif tx in srkey.Genus.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.Genus==tx].values[0]]\n",
    "    elif tx in srkey.Family.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.Family==tx].values[0]]\n",
    "    elif tx in srkey.Order.to_numpy():\n",
    "        tmp_group += [srkey.group[srkey.Order==tx].values[0]]\n",
    "    elif tx in ['Sphyrnidae','Selachimorpha']:\n",
    "        tmp_group += ['sharks']\n",
    "    elif tx in ['Elasmobranchii']:\n",
    "        tmp_group += ['elasmos']\n",
    "    else:\n",
    "        print(tx)\n",
    "fdata['group'] = np.array(tmp_group)[match(fdata.species,list(tmp_taxon))]\n",
    "\n",
    "\n",
    "\n",
    "## CHECK THAT ADDITIONAL ELASMOS ARE OK WITH RE-EXPORT CALCULATIONS. \n",
    "## CHECK ALL LANDINGS AND TRADE DATA TO ENSURE SAME YEAR-LEVEL OBSERVATIONS\n",
    "\n",
    "\n",
    "# ## Import commodity code table\n",
    "\n",
    "# Import taxonomic match table for BACI commodity codes and species (MASK)\n",
    "cdata = pd.read_csv(bd + \"comm.code.taxon.match.csv\")\n",
    "\n",
    "# ## Load BACI keys\n",
    "\n",
    "# Import BACI commodity code key\n",
    "ckey = pd.read_csv(bd + \"product_codes_HS12_V202102.csv\")\n",
    "ckey17 = pd.read_csv(bd + \"product_codes_HS17_V202102.csv\")\n",
    "\n",
    "# Import BACI country keys\n",
    "kdata = pd.read_csv(bd + \"country_codes_V202301.csv\")\n",
    "kdata.country_code = kdata.country_code.values.astype(int)\n",
    "\n",
    "# TWN doesn't have an ISO code\n",
    "kdata.loc[\n",
    "    kdata.country_name_full == \"Other Asia, not elsewhere specified\", \"iso_3digit_alpha\"\n",
    "] = \"TWN\"\n",
    "\n",
    "\n",
    "# ## Load BACI seafood trade\n",
    "\n",
    "# Import overall trade from BACI data\n",
    "odata = pd.read_csv(bd + \"baci.seafood_12-19_ij_all.csv\")\n",
    "\n",
    "# Make them numeric\n",
    "odata.exporter_i = odata.exporter_i.values.astype(int)\n",
    "odata.importer_j = odata.importer_j.values.astype(int)\n",
    "\n",
    "# Add country codes\n",
    "\n",
    "odata[\"ISOex_i\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(odata.exporter_i.values), list(kdata.country_code.values))\n",
    "]\n",
    "odata[\"ISOim_j\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(odata.importer_j.values), list(kdata.country_code.values))\n",
    "]\n",
    "\n",
    "\n",
    "# ## Load BACI meat trade\n",
    "\n",
    "# Import BACI data\n",
    "#tdata = pd.read_csv(bd + \"baci.elasmo_HS12_2012-2017.csv\")\n",
    "tdata = pd.read_csv(bd + \"baci_HS12-19_elasmo.csv\")\n",
    "tdata = tdata[tdata['quantity_q'].notna()]\n",
    "tdata = tdata[tdata['quantity_q']!='           NA']\n",
    "\n",
    "# Make them numeric\n",
    "tdata.exporter_i = tdata.exporter_i.values.astype(int)\n",
    "tdata.importer_j = tdata.importer_j.values.astype(int)\n",
    "tdata.quantity_q = tdata.quantity_q.values.astype(float)\n",
    "\n",
    "# Temporary change of code for Italy\n",
    "tdata.loc[tdata.exporter_i == 381, \"exporter_i\"] = 380\n",
    "tdata.loc[tdata.importer_j == 381, \"importer_j\"] = 380\n",
    "\n",
    "# Add country names for imports/exports\n",
    "tdata[\"ISOex_i\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(tdata.exporter_i.values), list(kdata.country_code.values))\n",
    "]\n",
    "tdata[\"ISOim_j\"] = kdata.iso_3digit_alpha.values[\n",
    "    match(list(tdata.importer_j.values), list(kdata.country_code.values))\n",
    "]\n",
    "\n",
    "# Add explicit code descriptions\n",
    "tdata['hs_description'] = ckey17.description.values[match(tdata[\"hscode_k\"],list(ckey17.code.values))]\n",
    "\n",
    "# Drop all trade on code 30571\n",
    "tdata = tdata[tdata[\"hscode_k\"]!=30571]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make fresh/frozen fins dummy\n",
    "tmp = np.array([0]*len(tdata['hs_description']))\n",
    "tmp[tdata[\"hscode_k\"]==30292] = 1\n",
    "tmp[tdata[\"hscode_k\"]==30392] = 1\n",
    "tdata['fins'] = tmp\n",
    "\n",
    "# Make rays dummy\n",
    "tdata['rays'] = np.array(['rays' in l for l in tdata['hs_description']])*1\n",
    "# Make 30488 sharks\n",
    "maskx = tdata.hscode_k==30488\n",
    "tdata.loc[maskx,'rays'] = 0\n",
    "\n",
    "# Add commodity group\n",
    "tmp = np.array(['sharks']*len(tdata['rays']))\n",
    "tmp[tdata['rays']==1] = 'rays'\n",
    "tdata['group'] = tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab sharks in years that have fresh/frozen fins separated\n",
    "tdata_17 = tdata.copy()[(tdata.year_t>=tdata[tdata.fins==1].year_t.min()) & (tdata.group=='sharks')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get total exports of fins and meat per year, exporter, importer combo\n",
    "tmp = tdata_17.groupby([\"ISOex_i\",\"ISOim_j\",\"year_t\",\"fins\"]).sum().reset_index()\n",
    "# Grab fins only\n",
    "tmp_fins = tmp.copy()[tmp.fins==1]\n",
    "# Sum over total per exporter, importer combo\n",
    "tmp_sum = tmp[[\"ISOex_i\",\"ISOim_j\",\"quantity_q\"]].groupby([\"ISOex_i\",\"ISOim_j\"]).sum()\n",
    "# Sum fins over total per year, exporter, importer\n",
    "tmp_fins_sum = tmp_fins[[\"ISOex_i\",\"ISOim_j\",\"quantity_q\"]].groupby([\"ISOex_i\",\"ISOim_j\"]).sum()\n",
    "# Grab proportion of fins where there are any\n",
    "tmp_fin_props = (tmp_fins_sum/tmp_sum).reset_index()\n",
    "# Make zero fins where there are none\n",
    "maskx = tmp_fin_props.quantity_q.isna()\n",
    "tmp_fin_props.loc[maskx,'quantity_q'] = 0\n",
    "# Convert to proportions of meat\n",
    "tmp_meat_props = tmp_fin_props.copy()\n",
    "tmp_meat_props['prop_sharks'] = 1-tmp_meat_props.quantity_q\n",
    "tmp_meat_props = tmp_meat_props.drop(columns=['quantity_q'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot average proportions of shark exports that are fins\n",
    "#plt.hist(tmp_fin_props.quantity_q[tmp_fin_props.quantity_q>0]);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "tdata_old = tdata.copy()\n",
    "# Iterate over exporter/importer combos\n",
    "for i in range(tmp_meat_props.shape[0]):\n",
    "    exi = tmp_meat_props.ISOex_i[i]\n",
    "    imi = tmp_meat_props.ISOim_j[i]\n",
    "    shark_prop = tmp_meat_props.prop_sharks[i]\n",
    "    # Re-scale observed sharks pre 2017 to remove potential fins\n",
    "    maskx = ((tdata.year_t<2017) & (tdata.ISOex_i==exi) & (tdata.ISOim_j==imi)&(tdata.group=='sharks'))\n",
    "    tdata.loc[maskx,'quantity_q'] = tdata.loc[maskx,'quantity_q']*shark_prop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.scatter(tdata_old.quantity_q,tdata.quantity_q)\n",
    "#plt.xlabel('All shark trade')\n",
    "#plt.ylabel('Adjusted shark trade')\n",
    "#plt.title('Shark trade with fins removed 2012-2017');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to live weights\n",
    "tdata['estimated_live_weight'] = tdata.quantity_q\n",
    "tmpmask = tdata.group=='sharks'\n",
    "tdata.loc[tmpmask,'estimated_live_weight'] = tdata.loc[tmpmask,'estimated_live_weight']*2\n",
    "tmpmask = tdata.group=='rays'\n",
    "tdata.loc[tmpmask,'estimated_live_weight'] = tdata.loc[tmpmask,'estimated_live_weight']*1.6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#plt.hist(tdata[tdata.hscode_k==30488].estimated_live_weight);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Reset biggest_countries to include only those with landings\n",
    "biggest_countries = country_\n",
    "\n",
    "# - - - - - - - - - - - Add BACI total seafood trade\n",
    "total_seafood_trade = (\n",
    "    odata[\n",
    "        ((odata.ISOex_i).isin(biggest_countries))\n",
    "        & ((odata.ISOim_j).isin(biggest_countries))\n",
    "    ]\n",
    "    .groupby([\"ISOex_i\", \"ISOim_j\"])\n",
    "    .sum()[\"ij_total\"]\n",
    "    .reset_index()\n",
    "    .set_index(\"ISOex_i\")\n",
    "    .pivot(columns=\"ISOim_j\")\n",
    "    .droplevel(0, axis=\"columns\")\n",
    "    .fillna(0.0)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deal with re-exports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# # REEXPORTS - NEED TO CHECK THAT RAYS ARE COOL HERE TOO\n",
    "#\n",
    "# Currently removes trade that has no possible catch.\n",
    "#\n",
    "# NB:\n",
    "#\n",
    "# 1. Assumes catches in year t are traded in year t\n",
    "\n",
    "# Pre-removals copy\n",
    "tdata_copy = tdata.copy()\n",
    "\n",
    "# Empty list of identified re-exports\n",
    "tmp = []\n",
    "# Unique country list\n",
    "tmp_c = np.unique(np.array(list(tdata.ISOex_i)+list(tdata.ISOim_j)))\n",
    "# Unique commodity codes\n",
    "tmp_u = tdata.group.unique()\n",
    "# Landings per country per year per commodity\n",
    "tmp_l = fdata.groupby([\"country_code\",\"year\",\"group\"]).sum().reset_index().sort_values(\"landed_weight\", ascending=False)\n",
    "\n",
    "# ====================== Remove re-exports from trade =========================== #\n",
    "# Iterate over years\n",
    "for y in tdata.year_t.unique():\n",
    "    # Grab values for year y \n",
    "    trad_ = tdata[tdata.year_t==y].groupby([\"ISOex_i\",'year_t','group']).sum().reset_index().sort_values(\"estimated_live_weight\", ascending=False)\n",
    "    # Grab total trade\n",
    "    trad = trad_.groupby([\"ISOex_i\"]).sum().reset_index().sort_values(\"estimated_live_weight\", ascending=False)\n",
    "    # Grab group trade\n",
    "    trad_s = trad[trad.group=='sharks']\n",
    "    trad_r = trad[trad.group=='rays']\n",
    "    # Grab total landings for year y\n",
    "    land = tmp_l[tmp_l.year==y]\n",
    "    # Grab possible group landings for year y\n",
    "    land_s = land[land.group=='sharks']\n",
    "    land_r = land[land.group=='rays']\n",
    "    land_e = land[land.group=='elasmos']\n",
    "    # Iteratre over countries\n",
    "    for e in tmp_c:\n",
    "        if e in land.country_code.unique() and e in trad.ISOex_i.unique():\n",
    "            # Grab values for exporter e in year y\n",
    "            tx = trad[trad.ISOex_i==e]\n",
    "            tx_s = trad_s[trad_s.ISOex_i==e]\n",
    "            tx_r = trad_r[trad_r.ISOex_i==e]\n",
    "            # Grab landings\n",
    "            lx = land[land.country_code==e]\n",
    "            lx_s = land_s[land_s.country_code==e]\n",
    "            lx_r = land_r[land_r.country_code==e]\n",
    "            lx_e = land_e[land_e.country_code==e]\n",
    "\n",
    "            # If no catches to support trade, make trade zero to remove re-exports\n",
    "            # Do this first because no catches of any kind trump group specifics\n",
    "            # justified because the project is about assigning catches within the trade to specific nations\n",
    "            if sum(lx.landed_weight)==0 and sum(tx.estimated_live_weight)>0:\n",
    "                tmp += [sum(tx.estimated_live_weight)]\n",
    "                maskx = (tdata.ISOex_i==e) & (tdata.year_t==y)\n",
    "                tdata.loc[maskx,'estimated_live_weight'] = 0\n",
    "                \n",
    "            # If no catches (group+elasmos) to support shark trade, make trade zero to remove re-exports: \n",
    "            if sum(lx_s.landed_weight+lx_e.landed_weight)==0 and sum(tx_s.estimated_live_weight)>0:\n",
    "                tmp += [sum(tx_s.estimated_live_weight)]\n",
    "                maskx = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='sharks')\n",
    "                tdata.loc[maskx,'estimated_live_weight'] = 0\n",
    "                \n",
    "            # If no catches (group+elasmos) to support ray trade, make trade zero to remove re-exports:\n",
    "            if sum(lx_r.landed_weight+lx_e.landed_weight)==0 and sum(tx_r.estimated_live_weight)>0:\n",
    "                tmp += [sum(tx_r.estimated_live_weight)]\n",
    "                maskx = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='rays')\n",
    "                tdata.loc[maskx,'estimated_live_weight'] = 0\n",
    "                \n",
    "            # If trade more than catches, make proportional within allowable group\n",
    "            elif sum(lx.landed_weight)<sum(tx.estimated_live_weight):\n",
    "                tmp += [sum(tx.estimated_live_weight)]\n",
    "                # Grab proportion\n",
    "                rrx = sum(lx.landed_weight)/sum(tx.estimated_live_weight)\n",
    "                # Re-scale trade to proportion of total landings\n",
    "                mask_s = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='sharks')\n",
    "                tdata.loc[mask_s,'estimated_live_weight'] = tdata.loc[mask_s,'estimated_live_weight']*rrx\n",
    "                mask_r = (tdata.ISOex_i==e) & (tdata.year_t==y) & (tdata.group=='rays')\n",
    "                tdata.loc[mask_r,'estimated_live_weight'] = tdata.loc[mask_r,'estimated_live_weight']*rrx\n",
    "        else:\n",
    "            mask = ((tdata.ISOex_i==e) & (tdata.year_t==y))\n",
    "            tdata.loc[mask,'estimated_live_weight'] = 0\n",
    "            #print(e,y)       \n",
    "\n",
    "# ## Restrict landings, seafood trade and meat trade to biggest countries\n",
    "\n",
    "# Original\n",
    "tmp_x = (tdata_copy.estimated_live_weight)\n",
    "# Updated\n",
    "tmp_y = (tdata.estimated_live_weight)\n",
    "# Countries with trade reduced\n",
    "iredux = (tdata_copy.estimated_live_weight-tdata.estimated_live_weight)>0\n",
    "# Grap re-exports data\n",
    "ReExports = pd.DataFrame(zip(tmp_x[iredux],tmp_y[iredux],tdata.ISOex_i[iredux].to_numpy(),tdata_copy.year_t[iredux],tdata_copy.group[iredux]),columns=['Original','Reduced','exporter','year','group'])\n",
    "ReExports['Net_diff'] = ReExports.Original-ReExports.Reduced\n",
    "ReExports['Exporter'] = kdata.country_name_abbreviation[[list(kdata.iso_3digit_alpha).index(x) for x in ReExports.exporter]].to_numpy()\n",
    "\n",
    "# Table of re-exporting countries\n",
    "tmp = ReExports.groupby(['Exporter']).sum().sort_values(by='Net_diff',ascending=False).drop(columns='exporter')\n",
    "tmp.to_csv('ReExport_totals.csv')\n",
    "# Unique local object\n",
    "ReExports_OBS = ReExports\n",
    "\n",
    "# = = = = = = = = = = = = = #\n",
    "# Remove zeros from trade\n",
    "tdata = tdata[tdata.estimated_live_weight!=0]\n",
    "\n",
    "# = = = = = = = = = = = = = #\n",
    "# Remove fins\n",
    "tdata = tdata[tdata['product']=='meat']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup landings data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split out shark and ray trade\n",
    "tdata_cut = tdata[\n",
    "    ((tdata.ISOex_i).isin(biggest_countries))\n",
    "    & ((tdata.ISOim_j).isin(biggest_countries))\n",
    "]\n",
    "# dropping missing values\n",
    "tdata_cut = tdata_cut.drop(\"Unnamed: 0\", axis=\"columns\").dropna().reset_index(drop=True)\n",
    "# Summarize data by importer export commodity\n",
    "tdata_cut = tdata_cut.groupby([\"ISOex_i\", \"ISOim_j\", \"group\"]).sum().reset_index()\n",
    "\n",
    "\n",
    "# Get average per year\n",
    "tdata_cut['estimated_live_weight'] = tdata_cut['estimated_live_weight']/len(tdata[\"year_t\"].unique())\n",
    "\n",
    "# we just care about whether it's a shark or ray, not if it's also fresh or frozen\n",
    "tdata_cut[\"fish_type\"] = tdata_cut[\"group\"]\n",
    "\n",
    "tdata_cut = tdata_cut.sort_values([\"ISOex_i\", \"ISOim_j\", \"fish_type\"])\n",
    "\n",
    "# Shark data\n",
    "tdata_cut_sharks = (\n",
    "    tdata_cut[tdata_cut.fish_type == \"sharks\"]\n",
    "    .groupby([\"ISOex_i\", \"ISOim_j\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Ray data\n",
    "tdata_cut_rays = (\n",
    "    tdata_cut[tdata_cut.fish_type == \"rays\"]\n",
    "    .groupby([\"ISOex_i\", \"ISOim_j\"])\n",
    "    .sum()\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/61/kzz9zs3s1gdcy367fbq9v3740000gn/T/ipykernel_40207/2171928780.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unreliability_score = pd.concat(\n",
      "/var/folders/61/kzz9zs3s1gdcy367fbq9v3740000gn/T/ipykernel_40207/2171928780.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unreliability_score = pd.concat(\n",
      "/var/folders/61/kzz9zs3s1gdcy367fbq9v3740000gn/T/ipykernel_40207/2171928780.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unreliability_score = pd.concat(\n",
      "/var/folders/61/kzz9zs3s1gdcy367fbq9v3740000gn/T/ipykernel_40207/2171928780.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unreliability_score = pd.concat(\n",
      "/var/folders/61/kzz9zs3s1gdcy367fbq9v3740000gn/T/ipykernel_40207/2171928780.py:33: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  unreliability_score = pd.concat(\n"
     ]
    }
   ],
   "source": [
    "## Load unreliability score\n",
    "\n",
    "unreliability_score = pd.read_csv(\n",
    "    bd + \"reporter_reliability_HS12_V202102.csv\",\n",
    "    usecols=[\"c\", \"q_unreliability_i\", \"q_unreliability_j\"],\n",
    ")\n",
    "\n",
    "# ITA changed code -- because, why not?\n",
    "unreliability_score.loc[unreliability_score.c == 381, \"c\"] = 380\n",
    "\n",
    "# grab ISO codes from odata\n",
    "unreliability_score = unreliability_score.rename(\n",
    "    columns={\"q_unreliability_i\": \"exporter\", \"q_unreliability_j\": \"importer\"}\n",
    ").merge(odata, left_on=\"c\", right_on=\"exporter_i\")\n",
    "unreliability_score = (\n",
    "    unreliability_score[\n",
    "        ((unreliability_score.ISOex_i).isin(biggest_countries))\n",
    "        & ((unreliability_score.ISOim_j).isin(biggest_countries))\n",
    "    ][[\"ISOex_i\", \"exporter\", \"importer\"]]\n",
    "    .groupby(\"ISOex_i\")\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Check for missing unreliability scores\n",
    "misx = biggest_countries[\n",
    "    np.array([x not in unreliability_score.index for x in biggest_countries])\n",
    "]\n",
    "\n",
    "# Fill missing unreliablity scores with maximum unreliability value\n",
    "tmp_maxval = max(unreliability_score.exporter)\n",
    "if len(misx) > 0:\n",
    "    for i in misx:\n",
    "        unreliability_score = pd.concat(\n",
    "            [\n",
    "                pd.DataFrame(index=[i], columns=unreliability_score.columns),\n",
    "                unreliability_score,\n",
    "            ]\n",
    "        )\n",
    "unreliability_score = unreliability_score.sort_index().fillna(tmp_maxval)\n",
    "unreliability_score = unreliability_score.reindex(\n",
    "    sorted(unreliability_score.columns), axis=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup taxon masking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count taxon aggregations\n",
    "ntax_country = (\n",
    "    txdata.groupby(by=([\"country\", \"taxon\"]))\n",
    "    .sum()\n",
    "    .reset_index()\n",
    "    .groupby(\"country\")\n",
    "    .count()\n",
    "    .taxon\n",
    ")\n",
    "\n",
    "# Add Belize\n",
    "ntax_country[\"BLZ\"] = 0\n",
    "# Re-order to match country_\n",
    "ntax_country = ntax_country[country_]\n",
    "\n",
    "# Taxon by country groupings\n",
    "taxindx1 = (ntax_country <= 1).to_numpy()\n",
    "taxindx2 = (ntax_country == 2).to_numpy()\n",
    "taxindx3 = (ntax_country >= 3).to_numpy()\n",
    "\n",
    "# Create 3 dimensional mask\n",
    "TaxonMASK_t1 = TaxonMASK_Sx.copy()\n",
    "TaxonMASK_t2 = TaxonMASK_Sx.copy()\n",
    "TaxonMASK_t3 = TaxonMASK_Sx.copy()\n",
    "\n",
    "# Deactivate countries without <=1, 2, or >=3 taxon groups reported\n",
    "TaxonMASK_t1[taxindx1 == False, ...] = 0\n",
    "TaxonMASK_t2[taxindx2 == False, ...] = 0\n",
    "TaxonMASK_t3[taxindx3 == False, ...] = 0\n",
    "\n",
    "# Make sure Elasmos bin is positive in countries with no aggregations\n",
    "NoTaxAgg = (NoTaxaSppWT.sum(1)==0)*1\n",
    "TaxonMASK_Sx[NoTaxAgg!=1,:,list(taxon_shortlist).index('Elasmobranchii')] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define `dims` and `coords`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# some countries can be missing from importers or exporters\n",
    "# indexing needs to take that into account\n",
    "# if we used `factorize`, that would be ignored\n",
    "\n",
    "country_to_idx_map = {country: index for index, country in enumerate(biggest_countries)}\n",
    "shark_exporter_idx = tdata_cut_sharks[\"ISOex_i\"].map(country_to_idx_map).to_numpy()\n",
    "shark_importer_idx = tdata_cut_sharks[\"ISOim_j\"].map(country_to_idx_map).to_numpy()\n",
    "ray_exporter_idx = tdata_cut_rays[\"ISOex_i\"].map(country_to_idx_map).to_numpy()\n",
    "ray_importer_idx = tdata_cut_rays[\"ISOim_j\"].map(country_to_idx_map).to_numpy()\n",
    "\n",
    "# You have to be careful when creating shark_trade_matrix:\n",
    "shark_trade_matrix = (\n",
    "    tdata_cut_sharks[[\"ISOex_i\", \"ISOim_j\", \"estimated_live_weight\"]]\n",
    "    .set_index(\"ISOex_i\")\n",
    "    .pivot(columns=\"ISOim_j\")\n",
    "    .droplevel(0, axis=\"columns\")\n",
    ")\n",
    "\n",
    "# Add missing exporters and importers\n",
    "missing_col = []\n",
    "for p in country_:\n",
    "    if not p in shark_trade_matrix.columns.values:\n",
    "        missing_col.append(p)\n",
    "missing_col = np.array(missing_col)\n",
    "shark_trade_matrix[missing_col] = np.NaN\n",
    "shark_trade_matrix = shark_trade_matrix[country_]\n",
    "missing_row = shark_trade_matrix.columns.difference(shark_trade_matrix.index)\n",
    "shark_trade_matrix = shark_trade_matrix.T\n",
    "shark_trade_matrix[missing_row] = np.NaN\n",
    "shark_trade_matrix = shark_trade_matrix[country_]\n",
    "shark_trade_matrix = shark_trade_matrix.T.sort_index().fillna(0)\n",
    "shark_trade_mask = shark_trade_matrix.to_numpy()\n",
    "shark_trade_mask[shark_trade_mask>0] = 1\n",
    "# Add domestic consumption\n",
    "np.fill_diagonal(shark_trade_mask,1)\n",
    "\n",
    "# You have to be careful when creating ray_trade_matrix:\n",
    "ray_trade_matrix = (\n",
    "    tdata_cut_rays[[\"ISOex_i\", \"ISOim_j\", \"estimated_live_weight\"]]\n",
    "    .set_index(\"ISOex_i\")\n",
    "    .pivot(columns=\"ISOim_j\")\n",
    "    .droplevel(0, axis=\"columns\")\n",
    ")\n",
    "\n",
    "# Add missing exporters and importers\n",
    "#missing_col = ray_trade_matrix.index.difference(ray_trade_matrix.columns)\n",
    "missing_col = []\n",
    "for p in country_:\n",
    "    if not p in ray_trade_matrix.columns.values:\n",
    "        missing_col.append(p)\n",
    "missing_col = np.array(missing_col)\n",
    "ray_trade_matrix[missing_col] = np.NaN\n",
    "ray_trade_matrix = ray_trade_matrix[country_]\n",
    "missing_row = ray_trade_matrix.columns.difference(ray_trade_matrix.index)\n",
    "ray_trade_matrix = ray_trade_matrix.T\n",
    "ray_trade_matrix[missing_row] = np.NaN\n",
    "ray_trade_matrix = ray_trade_matrix[country_]\n",
    "ray_trade_matrix = ray_trade_matrix.T.sort_index().fillna(0)\n",
    "ray_trade_mask = ray_trade_matrix.to_numpy()\n",
    "ray_trade_mask[ray_trade_mask>0] = 1\n",
    "# Add domestic consumption\n",
    "np.fill_diagonal(ray_trade_mask,1)\n",
    "\n",
    "# Species mask for possible trade (including domestic)\n",
    "trade_mask = ray_trade_mask[:,None,:]*((group_=='rays')[None,:,None])+shark_trade_mask[:,None,:]*((group_=='sharks')[None,:,None])\n",
    "trade_mask[trade_mask==0] = -999\n",
    "trade_mask[trade_mask>0] = 0\n",
    "# Remove species not used for meat\n",
    "#trade_mask = trade_mask+meat_mask[None,:,None]\n",
    "trade_mask[trade_mask<0] = -999\n",
    "\n",
    "# Mask for trade softmax to zero out species with all -999\n",
    "NoSPP_Mask = (((trade_mask==-999).sum(2)!=len(country_))*1)\n",
    "\n",
    "# Mask for blue shark relative odds importer preferences\n",
    "BSmask = np.zeros(shape=trade_mask[0].shape)\n",
    "BSmask[list(species_).index('Prionace glauca')] = -999\n",
    "\n",
    "# Better country labels\n",
    "biggest_countries_long = kdata.country_name_abbreviation[\n",
    "    [list(kdata.iso_3digit_alpha).index(x) for x in biggest_countries]\n",
    "].to_numpy()\n",
    "\n",
    "# Create matching tensor for priors\n",
    "SppPRIORadj_idx = SppPRIORadj.copy()\n",
    "# List of unique prior values\n",
    "priors_ = list(np.sort(np.unique(SppPRIORadj)))\n",
    "# Replace prior values with index to OddsCAT\n",
    "for i in range(len(SppPRIORadj_idx)):\n",
    "    SppPRIORadj_idx[i] = match(SppPRIORadj_idx[i],priors_)\n",
    "\n",
    "# Observed data country index \n",
    "obs_exporter_idx = np.array([country_to_idx_map[x] for x in Obscou])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = ((SppPRIOR[obs_exporter_idx,:]==-999) & (ObsLandings.values>0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in range(len(tmp.sum(1))):\n",
    "    if tmp[t].sum()>0:\n",
    "        tmpc = ObsLandings.country[t].values\n",
    "        tmps = ObsLandings.species[tmp[t]].values\n",
    "        tmpo = ObsLandings.sel(country=tmpc,species=tmps).values\n",
    "        tmpm = priorImportance.sel(country=tmpc,species=tmps).values\n",
    "        #print(tmpc,tmps,tmpo,tmpm)\n",
    "        # tmp fix of priors\n",
    "        SppPRIOR[country_==tmpc,np.isin(species_,tmps)] = 1\n",
    "        SppPRIORadj[country_==tmpc,np.isin(species_,tmps)] = 1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "COORDS = {\n",
    "    \"exporter\": biggest_countries,\n",
    "    \"importer\": biggest_countries,\n",
    "    \"obs_exporter\": country_[obs_exporter_idx],\n",
    "    \"shark_obs_idx\": tdata_cut_sharks.index,\n",
    "    \"ray_obs_idx\": tdata_cut_rays.index,\n",
    "    \"direction\": [\"exports\", \"imports\"],\n",
    "    \"quantity\": [\"weight\", \"value\"],\n",
    "    \"species\": species_,\n",
    "    \"landing_country\": country_,\n",
    "    \"taxon\": taxon_shortlist,\n",
    "    \"year\":year_,\n",
    "    \"OddsCAT\":np.unique(SppPRIORadj).astype(str)\n",
    "}\n",
    "print(\"Data loaded!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
