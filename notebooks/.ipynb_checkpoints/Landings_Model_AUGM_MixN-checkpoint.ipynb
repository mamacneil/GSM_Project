{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model for shark and ray meat landings\n",
    "\n",
    "This code implements a probabalistic model to estimate the proportions of species landed among major shark fishing and trading nations. The model uses expert opinion as prior information to estimate the total latent landings in any given country. Model developmenet has been primarily from Aaron MacNeil, Beth Babcock, Chris Mull, and Alex Andorra.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pymc as pm\n",
    "import pytensor.tensor as pyt\n",
    "import seaborn as sns\n",
    "import pdb\n",
    "from matplotlib.gridspec import GridSpec\n",
    "import xarray as xr\n",
    "import xarray_einstats\n",
    "import rdata as rd\n",
    "import mcbackend\n",
    "import clickhouse_driver\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set figure style.\n",
    "az.style.use(\"arviz-darkgrid\")\n",
    "# point to data and figure directories\n",
    "bd = os.getcwd() + \"/../Data/\"\n",
    "bf = os.getcwd() + \"/../Figures/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper functions\n",
    "def indexall(L):\n",
    "    poo = []\n",
    "    for p in L:\n",
    "        if not p in poo:\n",
    "            poo.append(p)\n",
    "    Ix = np.array([poo.index(p) for p in L])\n",
    "    return poo, Ix\n",
    "\n",
    "\n",
    "# Helper functions\n",
    "match = lambda a, b: np.array([b.index(x) if x in b else None for x in a])\n",
    "\n",
    "def zscore(x):\n",
    "    return (x-np.mean(x))/np.std(x)\n",
    "\n",
    "def unique(series: pd.Series):\n",
    "    \"Helper function to sort and isolate unique values of a Pandas Series\"\n",
    "    return series.sort_values().unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded!\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "exec(open(\"Joint_Trade_Landings_Data_AUGM.py\").read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initalize backend\n",
    "#ch_client = clickhouse_driver.Client(host=\"129.173.118.118\", password='buKcek-qetsyj-pynci7', database='gsmtdb')\n",
    "#ch_client = clickhouse_driver.Client(host=\"129.173.118.118\", password='buKcek-qetsyj-pynci7', database='gsmtdb', send_receive_timeout = 7200, settings={'max_execution_time': 7200})\n",
    "# Backend object\n",
    "#ch_backend = mcbackend.ClickHouseBackend(ch_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index cutoff - removes impossible -999 species from likelihood\n",
    "isppx = SppPRIOR > -5\n",
    "# Add +1 to possible species\n",
    "#ObsLandings_obs[isppx[obs_exporter_idx,:]] += 1\n",
    "# Observed totals for multinomial\n",
    "#ObsLandings_sums = ObsLandings.astype(int).sum(axis=1).values\n",
    "ObsLandings_sums = ObsLandings.sum(axis=1).values\n",
    "# Observed values for multinomial\n",
    "#ObsLandings_obs = ObsLandings.astype(int).values\n",
    "ObsLandings_obs = ObsLandings.values\n",
    "\n",
    "# Index for non-zeros\n",
    "zindx = ObsLandings_obs>0\n",
    "# Log data\n",
    "logObsLandings_obs = np.log(ObsLandings_obs[zindx])\n",
    "logObsLandings_obs2 = np.log(ObsLandings_obs+0.00001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set index cutoff - removes impossible -999 and non retained species from likelihood\n",
    "isppx = SppPRIOR > -5\n",
    "nspp = TaxonMASK_t3.shape[1]\n",
    "# Grab number of species available per country\n",
    "nspp_country = np.tile((1-(isppx==0)*1).sum(1),(nspp,1)).T\n",
    "\n",
    "# Grab taxon dims and numbers of aggregations per exporter\n",
    "ntax = TaxonMASK_t3.shape[2]\n",
    "ntaxon_country = np.tile(np.array([(x.sum(0)>0).sum() for x in TaxonMASK_t3]),(ntax,nspp,1)).T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false
   },
   "source": [
    "# Latent landings model - sharks and rays"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       ...,\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False],\n",
       "       [False, False, False, ..., False, False, False]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zindx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pm.Model(coords=COORDS) as landings_model_x:\n",
    "    # --------------------------------------------------------\n",
    "    #                     Data containers\n",
    "    # --------------------------------------------------------\n",
    "    \n",
    "    shark_exporter_id = pm.ConstantData(\n",
    "        \"shark_exporter_id\", shark_exporter_idx, dims=\"shark_obs_idx\"\n",
    "    )\n",
    "    shark_importer_id = pm.ConstantData(\n",
    "        \"shark_importer_id\", shark_importer_idx, dims=\"shark_obs_idx\"\n",
    "    )\n",
    "    ray_exporter_id = pm.ConstantData(\n",
    "        \"ray_exporter_id\", ray_exporter_idx, dims=\"ray_obs_idx\"\n",
    "    )\n",
    "    ray_importer_id = pm.ConstantData(\n",
    "        \"ray_importer_id\", ray_importer_idx, dims=\"ray_obs_idx\"\n",
    "    )\n",
    "    unreliability_data = pm.ConstantData(\n",
    "        \"unreliability_data\", unreliability_score[\"exporter\"], dims=\"exporter\"\n",
    "    )\n",
    "\n",
    "    # --------------------------------------------------------\n",
    "    #                   Landing data model\n",
    "    # --------------------------------------------------------\n",
    "    \n",
    "    # = = = = = = = = = = = = = = = = Latent Species  = = = = = = = = = = = = = = = = #\n",
    "\n",
    "    # Scale exporter variance to number of species\n",
    "    plo_intercept = pm.Normal('plo_intercept',np.log(3),2)\n",
    "    plo_numspp = pm.Normal('plo_numspp')\n",
    "    Spp_magnitude = pm.Deterministic('Spp_PLO', pyt.exp(plo_intercept+plo_numspp*np.log(nspp_country+0.0001)) )\n",
    "    \n",
    "    #'''\n",
    "    # National average log odds for latent landings by species\n",
    "    latent_logOdds_landings_ = pm.Normal(\"latent_logOdds_landings_\", SppPRIORadj[isppx], Spp_magnitude[isppx])\n",
    "    latent_logOdds_landings = pm.Deterministic(\n",
    "        \"latent_logOdds_landings\",\n",
    "        pyt.set_subtensor((pyt.ones(SppPRIOR.shape)*-9)[isppx], latent_logOdds_landings_),\n",
    "        dims=(\"exporter\", \"species\")\n",
    "    )\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Species proportions of total landings\n",
    "    SppProps = pm.Deterministic(\n",
    "        \"SppProps\",\n",
    "        pm.math.softmax(latent_logOdds_landings,axis=1),\n",
    "        dims=(\"exporter\", \"species\")\n",
    "    )\n",
    "    \n",
    "    # National average landings\n",
    "    Latent_landings = pm.Deterministic(\n",
    "        \"Latent_landings\",\n",
    "        SppProps*TotalCatch[:,None],\n",
    "        dims=(\"exporter\", \"species\")\n",
    "    )\n",
    "\n",
    "\n",
    "    \n",
    "    #\"\"\"\n",
    "    # = = = = = = = = = = = = = = = = OBSERVED DATA PROPORTIONS = = = = = = = = = = = = = = = = #\n",
    "\n",
    "    # National average landings\n",
    "    logLatent_landings_obs = pm.Deterministic(\n",
    "        \"logLatent_landings_obs\",\n",
    "        pyt.log(SppProps[obs_exporter_idx,:]*ObsLandings_sums[:,None]+0.00001),\n",
    "        dims=(\"obs_exporter\", \"species\")\n",
    "    )\n",
    "\n",
    "    # Uncertainty\n",
    "    ObsSD = pm.Exponential('ObsSD',1)\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal(\n",
    "        \"logObserved_landings\",\n",
    "        logLatent_landings_obs[zindx],\n",
    "        ObsSD,\n",
    "        observed=logObsLandings_obs,\n",
    "    )\n",
    "    \n",
    "    # = = = = = = = = = = = = = = = = SPECIES VS TAXON PROPORTIONS = = = = = = = = = = = = = = = = #\n",
    "\n",
    "    # log-odds for latent landings identified to species -\n",
    "    # CAN ADD COVARIATES HERE for TO SPP or AGGGREGATED E.G. GEAR, CITES, FISHERY, etc.\n",
    "    PsppIdent_odds = pm.Normal(\"PsppIdent_odds\", 0, 2, shape=SppPRIOR[isppx].shape)\n",
    "    PsppIdent_ = pm.Deterministic(\n",
    "        \"PsppIdent_ \", pm.math.invlogit(PsppIdent_odds + NoTaxaSppWT[isppx]*10)\n",
    "    )\n",
    "    # Expand to full rank - make impossibles guaranteed to be spp ID'd\n",
    "    PsppIdent = pm.Deterministic(\n",
    "        \"PsppIdent\",\n",
    "        pyt.set_subtensor(pyt.ones(SppPRIOR.shape)[isppx], PsppIdent_),\n",
    "        dims=(\"exporter\", \"species\"),\n",
    "    )\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = SPECIES Model = = = = = = = = = = = = = = = = #\n",
    "\n",
    "    # National average reported log landings by species\n",
    "    CountrySPP_landings = pm.Deterministic(\n",
    "        \"CountrySPP_landings\",\n",
    "        PsppIdent*Latent_landings,\n",
    "        dims=(\"exporter\", \"species\"),\n",
    "    )\n",
    "    \n",
    "    # National average reported log landings by species\n",
    "    CountrySPP_log_landings = pm.Deterministic(\n",
    "        \"CountrySPP_log_landings\",\n",
    "        pyt.log(CountrySPP_landings+0.00001),\n",
    "        dims=(\"exporter\", \"species\")\n",
    "    )\n",
    "    # Allow for annual landings variation\n",
    "    CountrySPP_log_yr = pm.ZeroSumNormal('CountrySPP_log_yr',dims=(\"exporter\", \"species\", \"year\"),n_zerosum_axes=2)\n",
    "\n",
    "    # Add on extra bit\n",
    "    CountrySPP_log_landings_yr = pm.Deterministic(\n",
    "        \"CountrySPP_log_landings_yr\",\n",
    "        CountrySPP_log_landings[:,:,None]+CountrySPP_log_yr,\n",
    "        dims=(\"exporter\", \"species\", \"year\")\n",
    "    )\n",
    "\n",
    "    # National reliability\n",
    "    lsd_intercept_sd_FAO = pm.Exponential(\"lsd_intercept_sd_FAO\", 1)\n",
    "    lsd_intercept_FAO = pm.Normal(\"lsd_intercept_FAO\", sigma=lsd_intercept_sd_FAO)\n",
    "\n",
    "    lsd_unreliability_sd_FAO = pm.Exponential(\"lsd_unreliability_sd_FAO\", 1)\n",
    "    lsd_unreliability_effect_FAO = pm.Normal(\n",
    "        #\"lsd_unreliability_effect_FAO\", sigma=lsd_unreliability_sd_FAO, dims=\"exporter\"\n",
    "        \"lsd_unreliability_effect_FAO\", sigma=lsd_unreliability_sd_FAO\n",
    "    )\n",
    "\n",
    "    reliability_FAO = pyt.exp(\n",
    "        lsd_intercept_FAO + lsd_unreliability_effect_FAO * unreliability_data\n",
    "    )\n",
    "    \n",
    "    # Likelihood\n",
    "    pm.Normal(\n",
    "        \"obs_spp\",\n",
    "        CountrySPP_log_landings_yr[country_spp_id, species_spp_id, year_spp_id],\n",
    "        reliability_FAO[country_spp_id],\n",
    "        observed=logReported_species_landings,\n",
    "    )\n",
    "\n",
    "    # = = = = = = = = = = = = = = = = TAXON MODEL = = = = = = = = = = = = = = = = #\n",
    "\n",
    "    # National average landings by species in taxon branch - remainder after species allocation\n",
    "    CountrySPP_Taxonlandings = pm.Deterministic(\n",
    "        \"CountrySPP_Taxonlandings\",\n",
    "        (Latent_landings-CountrySPP_landings)*NoTaxAgg[:,None],\n",
    "        dims=(\"exporter\", \"species\"),\n",
    "    )\n",
    "\n",
    "    # Scale taxon aggregations\n",
    "    taxon_intercept = pm.Normal('taxon_intercept', np.log(3), 2)\n",
    "    taxon_numspp = pm.Normal('taxon_numspp')\n",
    "    taxon_magnitude = pm.Deterministic('taxon_magnitude', pyt.exp(taxon_intercept+taxon_numspp*np.log(ntaxon_country+0.0001)) )\n",
    "    # log-odds for species contributions to taxon aggregations\n",
    "    TaxonAGG_odds_ = pm.Normal(\"TaxonAGG_odds_\", -1*TaxonMASK_Sx[TaxonMASK_Sx == 1], taxon_magnitude[TaxonMASK_Sx == 1])\n",
    "    # Expand to full rank - make impossibles zero for adding in -999 at next step\n",
    "    TaxonAGG_odds = pm.Deterministic(\n",
    "        \"TaxonAGG_odds\",\n",
    "        pyt.set_subtensor((pyt.ones(TaxonMASK_Sx.shape)*-9)[TaxonMASK_Sx == 1], TaxonAGG_odds_),\n",
    "        dims=(\"exporter\", \"species\", \"taxon\"),\n",
    "    )\n",
    "\n",
    "    # log-odds for species contributions to taxon aggregations\n",
    "    TaxonAGG_odds2_ = pm.Normal(\"TaxonAGG_odds2_\", -1*TaxonMASK_Sx[TaxonMASK_Sx == 1], 2)\n",
    "    # Expand to full rank - make impossibles zero for adding in -999 at next step\n",
    "    TaxonAGG_odds2 = pm.Deterministic(\n",
    "        \"TaxonAGG_odds2\",\n",
    "        pyt.set_subtensor((pyt.ones(TaxonMASK_Sx.shape)*-9)[TaxonMASK_Sx == 1], TaxonAGG_odds2_),\n",
    "        dims=(\"exporter\", \"species\", \"taxon\"),\n",
    "    )\n",
    "\n",
    "    # Softmax for allowable taxa in country - CAN ADD COVARIATES FOR TAXON CLASSIFICATION HERE IF EVER AVAILABLE; TaxonAGG_odds need\n",
    "    # be pyt.zeros and the refined step used if covariates added\n",
    "    #TaxonAGG_odds_refined = TaxonMASK_NEG*10 + TaxonAGG_odds\n",
    "    # Binomial probability for 2 reporting categories\n",
    "    TaxonAGG = pm.math.invlogit(TaxonAGG_odds2)\n",
    "    # Three probabilities for >3, 2, or <=1 reported taxa\n",
    "    P_TaxonSPP = pm.Deterministic(\n",
    "        \"P_TaxonSPP\",\n",
    "        # Softmax for exporters with 3 or more taxon aggregation categories; 0.4 centres proportions\n",
    "        pm.math.softmax(TaxonAGG_odds, axis=2)*taxindx3[:,None,None]\n",
    "        # Invlogit for exporters with 2 taxon aggregation categories - log sum to ensure unity\n",
    "        + pyt.exp(pyt.log(TaxonAGG)-pyt.log((pyt.sum(TaxonAGG, axis=2))[:, :, None]))*taxindx2[:,None,None]\n",
    "        # Unity for exporters reporting only one aggregation category\n",
    "        + TaxonMASK_t1,\n",
    "        dims=(\"exporter\", \"species\", \"taxon\"),\n",
    "    )\n",
    "\n",
    "    # Latent species within taxa\n",
    "    CountryTaxon_SPP_landings = pm.Deterministic(\n",
    "        \"CountryTaxon_SPP_landings\",\n",
    "        P_TaxonSPP*CountrySPP_Taxonlandings[:, :, None],\n",
    "        dims=(\"exporter\", \"species\", \"taxon\"),\n",
    "    )\n",
    "\n",
    "    # National average reported log landings by taxon\n",
    "    CountryTaxon_log_landings = pm.Deterministic(\n",
    "        \"CountryTaxon_log_landings\",\n",
    "        pyt.log(CountryTaxon_SPP_landings.sum(1)+0.0001),\n",
    "        dims=(\"exporter\", \"taxon\"),\n",
    "    )\n",
    "\n",
    "    # Allow for annual landings variation\n",
    "    CountryTaxon_log_yr = pm.ZeroSumNormal('CountryTaxon_log_yr',dims=(\"exporter\", \"taxon\", \"year\"),n_zerosum_axes=2)\n",
    "\n",
    "    # Add on extra bit\n",
    "    CountryTaxon_log_landings_yr = pm.Deterministic(\n",
    "        \"CountryTaxon_log_landings_yr\",\n",
    "        CountryTaxon_log_landings[:,:,None]+CountryTaxon_log_yr,\n",
    "        dims=(\"exporter\", \"taxon\", \"year\")\n",
    "    )\n",
    "\n",
    "    # Likelihood\n",
    "    pm.Normal(\n",
    "        \"obs_taxon\",\n",
    "        CountryTaxon_log_landings_yr[country_tax_id, taxon_tax_id, year_tax_id],\n",
    "        reliability_FAO[country_tax_id],\n",
    "        observed=logReported_taxon_landings,\n",
    "    )\n",
    "    #\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Only 50 samples in chain.\n",
      "Auto-assigning NUTS sampler...\n",
      "Initializing NUTS using jitter+adapt_diag...\n",
      "Multiprocess sampling (4 chains in 4 jobs)\n",
      "NUTS: [plo_intercept, plo_numspp, latent_logOdds_landings_, ObsSD, PsppIdent_odds, CountrySPP_log_yr, lsd_intercept_sd_FAO, lsd_intercept_FAO, lsd_unreliability_sd_FAO, lsd_unreliability_effect_FAO, taxon_intercept, taxon_numspp, TaxonAGG_odds_, TaxonAGG_odds2_, CountryTaxon_log_yr]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='15' class='' max='600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      2.50% [15/600 00:02&lt;01:19 Sampling 4 chains, 0 divergences]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Not enough samples to build a trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m landings_model_x:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m#pm.sample(draws=50, tune=200, trace=ch_backend, idata_kwargs=dict(log_likelihood=False))\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     idata_landings_x \u001b[38;5;241m=\u001b[39m \u001b[43mpm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdraws\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mlog_likelihood\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDone sampling\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/gsm-project/lib/python3.11/site-packages/pymc/sampling/mcmc.py:827\u001b[0m, in \u001b[0;36msample\u001b[0;34m(draws, tune, chains, cores, random_seed, progressbar, step, nuts_sampler, initvals, init, jitter_max_retries, n_init, trace, discard_tuned_samples, compute_convergence_checks, keep_warning_stat, return_inferencedata, idata_kwargs, nuts_sampler_kwargs, callback, mp_ctx, model, **kwargs)\u001b[0m\n\u001b[1;32m    823\u001b[0m t_sampling \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t_start\n\u001b[1;32m    825\u001b[0m \u001b[38;5;66;03m# Packaging, validating and returning the result was extracted\u001b[39;00m\n\u001b[1;32m    826\u001b[0m \u001b[38;5;66;03m# into a function to make it easier to test and refactor.\u001b[39;00m\n\u001b[0;32m--> 827\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_sample_return\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    828\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrun\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    829\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtraces\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    830\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtune\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtune\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    831\u001b[0m \u001b[43m    \u001b[49m\u001b[43mt_sampling\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mt_sampling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdiscard_tuned_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcompute_convergence_checks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    834\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_inferencedata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkeep_warning_stat\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    836\u001b[0m \u001b[43m    \u001b[49m\u001b[43midata_kwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43midata_kwargs\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    837\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    838\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniforge3/envs/gsm-project/lib/python3.11/site-packages/pymc/sampling/mcmc.py:858\u001b[0m, in \u001b[0;36m_sample_return\u001b[0;34m(run, traces, tune, t_sampling, discard_tuned_samples, compute_convergence_checks, return_inferencedata, keep_warning_stat, idata_kwargs, model)\u001b[0m\n\u001b[1;32m    856\u001b[0m \u001b[38;5;66;03m# Pick and slice chains to keep the maximum number of samples\u001b[39;00m\n\u001b[1;32m    857\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m discard_tuned_samples:\n\u001b[0;32m--> 858\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m \u001b[43m_choose_chains\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtraces\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtune\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    859\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    860\u001b[0m     traces, length \u001b[38;5;241m=\u001b[39m _choose_chains(traces, \u001b[38;5;241m0\u001b[39m)\n",
      "File \u001b[0;32m~/miniforge3/envs/gsm-project/lib/python3.11/site-packages/pymc/backends/base.py:601\u001b[0m, in \u001b[0;36m_choose_chains\u001b[0;34m(traces, tune)\u001b[0m\n\u001b[1;32m    599\u001b[0m lengths \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;28mlen\u001b[39m(trace) \u001b[38;5;241m-\u001b[39m tune) \u001b[38;5;28;01mfor\u001b[39;00m trace \u001b[38;5;129;01min\u001b[39;00m traces]\n\u001b[1;32m    600\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28msum\u001b[39m(lengths):\n\u001b[0;32m--> 601\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough samples to build a trace.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    603\u001b[0m idxs \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margsort(lengths)\n\u001b[1;32m    604\u001b[0m l_sort \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(lengths)[idxs]\n",
      "\u001b[0;31mValueError\u001b[0m: Not enough samples to build a trace."
     ]
    }
   ],
   "source": [
    "with landings_model_x:\n",
    "    #pm.sample(draws=50, tune=200, trace=ch_backend, idata_kwargs=dict(log_likelihood=False))\n",
    "    idata_landings_x = pm.sample(draws=50, tune=100, idata_kwargs=dict(log_likelihood=False))\n",
    "print('Done sampling')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample from prior and posterior predictive distributions\n",
    "with landings_model_x:\n",
    "    try:\n",
    "        idata_landings_x.extend(pm.sample_prior_predictive())\n",
    "        idata_landings_x.extend(pm.sample_posterior_predictive(idata_landings_x))\n",
    "        print('Done predictives')\n",
    "    except:\n",
    "        print('Failed predictives')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "# ArviZ doesn't handle MultiIndex yet\n",
    "# Making it aware of the real data labeling at the obs level\n",
    "biggest_countries_long = kdata.country_name_abbreviation[\n",
    "    [list(kdata.iso_3digit_alpha).index(x) for x in biggest_countries]\n",
    "].to_numpy()\n",
    "\n",
    "\"\"\"\n",
    "obs_idx_detailed = tdata_cut_sharks.set_index([\"ISOex_i\", \"ISOim_j\"]).index\n",
    "\n",
    "more_coords = {\n",
    "    \"obs_idx\": obs_idx_detailed,\n",
    "    \"ISOex_i\": (\"obs_idx\", tdata_cut_sharks[\"ISOex_i\"]),\n",
    "    \"ISOim_j\": (\"obs_idx\", tdata_cut_sharks[\"ISOim_j\"]),\n",
    "}\n",
    "\"\"\"\n",
    "\n",
    "more_coords = {\n",
    "    \"Exporter\": (\"exporter\", biggest_countries_long),\n",
    "    \"Importer\": (\"importer\", biggest_countries_long)\n",
    "}\n",
    "\n",
    "\n",
    "idata_landings_x.prior = idata_landings_x.prior.assign_coords(more_coords)\n",
    "idata_landings_x.prior_predictive = idata_landings_x.prior_predictive.assign_coords(more_coords)\n",
    "idata_landings_x.posterior = idata_landings_x.posterior.assign_coords(more_coords)\n",
    "idata_landings_x.posterior_predictive = idata_landings_x.posterior_predictive.assign_coords(\n",
    "    more_coords\n",
    ")\n",
    "idata_landings_x.observed_data = idata_landings_x.observed_data.assign_coords(more_coords)\n",
    "idata_landings_x.constant_data = idata_landings_x.constant_data.assign_coords(more_coords)\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results\n",
    "idata_landings_x.to_netcdf(\"idata-landings-model_AUGM.nc\")\n",
    "print(\"Saved trace!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab species level estimates for observed data\n",
    "Obs_spp_data = np.exp(sdata.drop(columns=['year','year_spp_id','country_spp_id','species_spp_id']\n",
    "          ).groupby(['country','species']).mean()).rename(columns={\"logReported_species_landings\": \"Reported_landings\"})\n",
    "\n",
    "# Create xarray data of totals by species\n",
    "Obs_spp_sums = xr.Dataset(Obs_spp_data.groupby(level='species').sum().sort_values(by='Reported_landings', ascending=False))\n",
    "\n",
    "# Grab taxon level estimates for observed data\n",
    "Obs_tax_data = np.exp(txdata.drop(columns=['year','year_tax_id','country_tax_id','taxon_tax_id']\n",
    "          ).groupby(['country','taxon']).mean()).rename(columns={\"logReported_taxon_landings\": \"Reported_landings\"})\n",
    "\n",
    "# Net estimated unreported species-level landings\n",
    "Net_spp_landings = idata_landings_x.posterior['Latent_landings']-idata_landings_x.posterior['CountrySPP_landings']\n",
    "Net_spp_landings_tax = idata_landings_x.posterior['CountrySPP_Taxonlandings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "mu_pp = idata_landings_x.posterior_predictive[\"obs_spp\"]\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(idata_landings_x.observed_data[\"obs_spp\"], mu_pp.mean((\"chain\", \"draw\")))\n",
    "az.plot_hdi(idata_landings_x.observed_data[\"obs_spp\"], mu_pp)\n",
    "ax.plot((0,12.5),(0,12.5),linestyle=\":\")\n",
    "ax.set_xlabel(\"Observed\")\n",
    "ax.set_ylabel(\"Expected\")\n",
    "ax.set_title(\"Observed vs Expected log Species landings\")\n",
    "plt.savefig(bf+'/Diagnostics/'+'Observed_vs_Expected_log_shark_trade.jpg',dpi=300);\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#\"\"\"\n",
    "mu_pp = idata_landings_x.posterior_predictive[\"obs_taxon\"]\n",
    "_, ax = plt.subplots()\n",
    "ax.scatter(idata_landings_x.observed_data[\"obs_taxon\"], mu_pp.mean((\"chain\", \"draw\")))\n",
    "az.plot_hdi(idata_landings_x.observed_data[\"obs_taxon\"], mu_pp)\n",
    "ax.plot((0,12.5),(0,12.5),linestyle=\":\")\n",
    "ax.set_xlabel(\"Observed\")\n",
    "ax.set_ylabel(\"Expected\")\n",
    "ax.set_title(\"Observed vs Expected log Taxon landings\")\n",
    "plt.savefig(bf+'/Diagnostics/'+'Observed_vs_Expected_log_shark_trade.jpg',dpi=300);\n",
    "#\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# = = = = = Set up plot\n",
    "_, ax = plt.subplots(1,2,figsize=(10, 15))\n",
    "\n",
    "# Select top 10% of landed species\n",
    "tmp = idata_landings_x.posterior['CountrySPP_landings'].sum(('exporter')).mean(('chain','draw'))\n",
    "tmp = tmp.sortby(tmp, ascending=False)\n",
    "sppx_landed = tmp[tmp>np.quantile(tmp,0.90)].species\n",
    "\n",
    "# Select top 10% of unreported species\n",
    "tmp = Net_spp_landings.sum(('exporter')).mean(('chain','draw'))\n",
    "tmp = tmp.sortby(tmp, ascending=False)\n",
    "sppx_unrep = tmp[tmp>np.quantile(tmp,0.90)].species\n",
    "\n",
    "# = = = = = Plot reported species landings\n",
    "az.plot_forest(idata_landings_x.posterior['CountrySPP_landings'].sum(('exporter')).sel(species=sppx_landed).rename(''),\n",
    "                    hdi_prob=0.9,\n",
    "                    #transform=np.exp,\n",
    "                    #figsize=(10, 20)\n",
    "               ax=ax[0]\n",
    "                   )\n",
    "ax[0].set(\n",
    "    title=\"Top reported landings\",\n",
    "    xlabel=\"tonnes\",\n",
    "    ylabel=\"\",\n",
    ");\n",
    "#ax[0].set_xlim(0,200000)\n",
    "for tick in ax[0].get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "# = = = = = Plot unreported species landings\n",
    "az.plot_forest(Net_spp_landings.sum(('exporter')).sel(species=sppx_unrep).rename(''),\n",
    "                    #transform=np.exp,\n",
    "                    #figsize=(10, 20)\n",
    "                    hdi_prob=0.9,\n",
    "                   ax=ax[1]\n",
    "                   )\n",
    "ax[1].set(\n",
    "    title=\"Top unreported landings\",\n",
    "    xlabel=\"tonnes\",\n",
    "    ylabel=\"\",\n",
    ")\n",
    "#ax[1].set_xlim(0,120000)\n",
    "for tick in ax[1].get_xticklabels():\n",
    "    tick.set_rotation(45)\n",
    "\n",
    "plt.savefig(bf+'/Global/'+'Reported_Unreported_species.jpg',dpi=300);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
